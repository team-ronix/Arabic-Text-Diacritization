{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T16:00:14.471788Z",
     "iopub.status.busy": "2025-11-26T16:00:14.470925Z",
     "iopub.status.idle": "2025-11-26T16:00:14.474963Z",
     "shell.execute_reply": "2025-11-26T16:00:14.474256Z",
     "shell.execute_reply.started": "2025-11-26T16:00:14.471753Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# GDRIVE_ID_DATA = \"1ONRQ36PFPnYNA4R6ZlmM7UQJ4LiAzEH0\"\n",
    "# !gdown $GDRIVE_ID_DATA -O Arabic-Text-Diacritization.zip\n",
    "# !unzip Arabic-Text-Diacritization.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDRIVE_ID_LSTM_MODEL = \"1kLRQ3o7m57qK1OJOTA-K9OBYL29zuXjo\"\n",
    "# !gdown $GDRIVE_ID_LSTM_MODEL -O LSTM.joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tarfile\n",
    "\n",
    "# file_path = \"/kaggle/input/tashkeela/Tashkeela-arabic-diacritized-text-utf8-0.3.tar.bz2\"\n",
    "# extract_path = \"/kaggle/working/tashkeela_extracted\"\n",
    "\n",
    "# # Extract tar.bz2 file\n",
    "# with tarfile.open(file_path, \"r:bz2\") as tar:\n",
    "#     tar.extractall(path=extract_path)\n",
    "\n",
    "# extract_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Arabic letters and diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T16:09:14.506351Z",
     "iopub.status.busy": "2025-11-26T16:09:14.505803Z",
     "iopub.status.idle": "2025-11-26T16:09:14.511024Z",
     "shell.execute_reply": "2025-11-26T16:09:14.510226Z",
     "shell.execute_reply.started": "2025-11-26T16:09:14.506323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import pyarabic.araby as araby\n",
    "import pyarabic.number as number\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import unicodedata\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Configuration for Kaggle\n",
    "Check GPU availability and configure TensorFlow to use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T16:00:18.118960Z",
     "iopub.status.busy": "2025-11-26T16:00:18.118691Z",
     "iopub.status.idle": "2025-11-26T16:00:18.125503Z",
     "shell.execute_reply": "2025-11-26T16:00:18.124653Z",
     "shell.execute_reply.started": "2025-11-26T16:00:18.118938Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "No GPU found. Training will use CPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Get GPU details\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth to avoid OOM errors\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        print(f\"GPU detected: {gpus}\")\n",
    "        print(f\"GPU Name: {tf.test.gpu_device_name()}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found. Training will use CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dictionaries and Create Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T16:00:22.440725Z",
     "iopub.status.busy": "2025-11-26T16:00:22.440119Z",
     "iopub.status.idle": "2025-11-26T16:00:22.446751Z",
     "shell.execute_reply": "2025-11-26T16:00:22.445974Z",
     "shell.execute_reply.started": "2025-11-26T16:00:22.440700Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "arabic_letters = []\n",
    "diacritics = []\n",
    "diacritics_to_id = {}\n",
    "with open('./utils/arabic_letters.pickle', 'rb') as f:\n",
    "    arabic_letters = pickle.load(f)\n",
    "with open('./utils/diacritics.pickle', 'rb') as f:\n",
    "    diacritics = pickle.load(f)\n",
    "with open('./utils/diacritic2id.pickle', 'rb') as f:\n",
    "    diacritics_to_id = pickle.load(f)\n",
    "\n",
    "arabic_letters_sorted = sorted(arabic_letters)\n",
    "char_to_id = {char: idx + 1 for idx, char in enumerate(arabic_letters_sorted)}\n",
    "char_to_id['<PAD>'] = 0\n",
    "char_to_id['UNK'] = len(char_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Read train and val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T14:42:21.867722Z",
     "iopub.status.busy": "2025-11-26T14:42:21.867488Z",
     "iopub.status.idle": "2025-11-26T14:42:22.056003Z",
     "shell.execute_reply": "2025-11-26T14:42:22.055417Z",
     "shell.execute_reply.started": "2025-11-26T14:42:21.867702Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "val_data = []\n",
    "with open('./data/train.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        train_data.append(line.strip())\n",
    "with open('./data/val.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        val_data.append(line.strip())\n",
    "print(len(train_data))\n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T14:42:22.057548Z",
     "iopub.status.busy": "2025-11-26T14:42:22.057282Z",
     "iopub.status.idle": "2025-11-26T14:42:22.061876Z",
     "shell.execute_reply": "2025-11-26T14:42:22.061144Z",
     "shell.execute_reply.started": "2025-11-26T14:42:22.057531Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_arabic_text(text):\n",
    "    \"\"\"\n",
    "    Clean text to keep only Arabic letters, diacritics, and spaces\n",
    "    \"\"\"\n",
    "    # Create a set of allowed characters (Arabic letters + diacritics + space)\n",
    "    allowed_chars = arabic_letters.union(diacritics, {' ', '\\t', '\\n'})\n",
    "    \n",
    "    # Filter the text to keep only allowed characters\n",
    "    cleaned_text = ''.join(char for char in text if char in allowed_chars)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def split_sentences(sentences, window_size=window_size):\n",
    "    all_segments = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = araby.tokenize(sentence)\n",
    "        current_segment = []\n",
    "        current_len = 0\n",
    "        \n",
    "        for word in words:\n",
    "            word_len = len(word)\n",
    "            add_space = 1 if current_segment else 0\n",
    "            \n",
    "            if current_len + word_len + add_space <= window_size:\n",
    "                current_segment.append(word)\n",
    "                current_len += word_len + add_space\n",
    "            else:\n",
    "                # save the segment\n",
    "                if current_segment:\n",
    "                    all_segments.append(\" \".join(current_segment))\n",
    "                \n",
    "                # start new segment\n",
    "                current_segment = [word]\n",
    "                current_len = word_len\n",
    "        \n",
    "        # append the final segment of the sentence\n",
    "        if current_segment:\n",
    "            all_segments.append(\" \".join(current_segment))\n",
    "\n",
    "    return all_segments\n",
    "\n",
    "\n",
    "\n",
    "def sentence_tokeniz(sentences):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        subsentences = araby.sentence_tokenize(sentence)\n",
    "        tokenized_sentences.extend(subsentences)\n",
    "    return tokenized_sentences\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T14:42:22.062899Z",
     "iopub.status.busy": "2025-11-26T14:42:22.062667Z",
     "iopub.status.idle": "2025-11-26T14:42:26.293937Z",
     "shell.execute_reply": "2025-11-26T14:42:26.293346Z",
     "shell.execute_reply.started": "2025-11-26T14:42:22.062874Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106.61171167507973\n"
     ]
    }
   ],
   "source": [
    "train_data = sentence_tokeniz(train_data)\n",
    "val_data = sentence_tokeniz(val_data)\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i] = clean_arabic_text(train_data[i])\n",
    "for i in range(len(val_data)):\n",
    "    val_data[i] = clean_arabic_text(val_data[i])\n",
    "\n",
    "train_data = split_sentences(train_data, window_size)\n",
    "val_data = split_sentences(val_data, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T14:42:26.296124Z",
     "iopub.status.busy": "2025-11-26T14:42:26.295649Z",
     "iopub.status.idle": "2025-11-26T14:42:26.300721Z",
     "shell.execute_reply": "2025-11-26T14:42:26.300019Z",
     "shell.execute_reply.started": "2025-11-26T14:42:26.296105Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def is_diacritic(ch):\n",
    "    # Unicode combining marks (Arabic diacritics are combining marks)\n",
    "    return unicodedata.combining(ch) != 0\n",
    "\n",
    "def extract_base_and_diacritics(text):\n",
    "    # normalize to NFC so base+combining marks are consistent\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    bases = []\n",
    "    diacs = []\n",
    "    current_base = None\n",
    "    current_diac = ''\n",
    "    for ch in text:\n",
    "        if is_diacritic(ch):\n",
    "            # accumulate diacritics for current base\n",
    "            current_diac += ch\n",
    "        else:\n",
    "            # new base character\n",
    "            if current_base is not None:\n",
    "                bases.append(current_base)\n",
    "                diacs.append(current_diac)\n",
    "            current_base = ch\n",
    "            current_diac = ''\n",
    "    # append last\n",
    "    if current_base is not None:\n",
    "        bases.append(current_base)\n",
    "        diacs.append(current_diac)\n",
    "    return bases, diacs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prepare data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T14:42:26.301783Z",
     "iopub.status.busy": "2025-11-26T14:42:26.301460Z",
     "iopub.status.idle": "2025-11-26T14:42:32.581555Z",
     "shell.execute_reply": "2025-11-26T14:42:32.580969Z",
     "shell.execute_reply.started": "2025-11-26T14:42:26.301760Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare training data - extract characters and their diacritics\n",
    "x_train_raw = []\n",
    "y_train_raw = []\n",
    "\n",
    "# Use a constant for unknown diacritic instead of hardcoded value\n",
    "UNKNOWN_DIACRITIC_ID = diacritics_to_id.get('', len(diacritics_to_id) - 1)\n",
    "\n",
    "for text in train_data:\n",
    "    bases, diacs = extract_base_and_diacritics(text)\n",
    "    # convert letters to IDs\n",
    "    x_train_raw.append([char_to_id.get(c, char_to_id['UNK']) for c in bases])\n",
    "    y_train_raw.append([diacritics_to_id.get(d, UNKNOWN_DIACRITIC_ID) for d in diacs])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T14:42:32.582514Z",
     "iopub.status.busy": "2025-11-26T14:42:32.582283Z",
     "iopub.status.idle": "2025-11-26T14:42:32.613906Z",
     "shell.execute_reply": "2025-11-26T14:42:32.613193Z",
     "shell.execute_reply.started": "2025-11-26T14:42:32.582488Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(char_to_id)\n",
    "num_diacritics = len(diacritics_to_id)\n",
    "\n",
    "model = models.Sequential([\n",
    "    # TRAINABLE EMBEDDINGS (THIS LAYER LEARNS)\n",
    "    layers.Embedding(input_dim=vocab_size,\n",
    "                     output_dim=128,     # embedding size (trainable)\n",
    "                     mask_zero=True),\n",
    "\n",
    "    # BiLSTM for sequence modeling\n",
    "    layers.Bidirectional(layers.LSTM(128, return_sequences=True)),\n",
    "\n",
    "    # Predict diacritic for each character - use Dense directly instead of TimeDistributed\n",
    "    layers.Dense(num_diacritics, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T14:42:32.614775Z",
     "iopub.status.busy": "2025-11-26T14:42:32.614592Z",
     "iopub.status.idle": "2025-11-26T14:42:34.461485Z",
     "shell.execute_reply": "2025-11-26T14:42:34.460857Z",
     "shell.execute_reply.started": "2025-11-26T14:42:32.614760Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (172285, 1093)\n",
      "y_train shape: (172285, 1093)\n"
     ]
    }
   ],
   "source": [
    "# Pad sequences to same length\n",
    "PAD_DIACRITIC_ID = diacritics_to_id.get('', 0)  # Use empty string diacritic for padding\n",
    "\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train_raw, padding='post', value=0)\n",
    "y_train = tf.keras.preprocessing.sequence.pad_sequences(y_train_raw, padding='post', value=PAD_DIACRITIC_ID)\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T14:42:34.462518Z",
     "iopub.status.busy": "2025-11-26T14:42:34.462210Z",
     "iopub.status.idle": "2025-11-26T15:31:15.645077Z",
     "shell.execute_reply": "2025-11-26T15:31:15.644274Z",
     "shell.execute_reply.started": "2025-11-26T14:42:34.462499Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Train model on GPU\n",
    "# with tf.device('/GPU:0'):\n",
    "    # history = model.fit(x_train, y_train, epochs=10, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T16:09:21.317266Z",
     "iopub.status.busy": "2025-11-26T16:09:21.316625Z",
     "iopub.status.idle": "2025-11-26T16:09:21.394527Z",
     "shell.execute_reply": "2025-11-26T16:09:21.393959Z",
     "shell.execute_reply.started": "2025-11-26T16:09:21.317240Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# joblib.dump(model, \"/kaggle/working/model1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"./models/LSTM.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T16:00:40.152198Z",
     "iopub.status.busy": "2025-11-26T16:00:40.151894Z",
     "iopub.status.idle": "2025-11-26T16:00:40.450841Z",
     "shell.execute_reply": "2025-11-26T16:00:40.450291Z",
     "shell.execute_reply.started": "2025-11-26T16:00:40.152175Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare validation data\n",
    "x_val_raw = []\n",
    "y_val_raw = []\n",
    "\n",
    "for text in val_data:\n",
    "    bases, diacs = extract_base_and_diacritics(text)\n",
    "    # convert letters to IDs\n",
    "    x_val_raw.append([char_to_id.get(c, char_to_id['UNK']) for c in bases])\n",
    "    y_val_raw.append([diacritics_to_id.get(d, UNKNOWN_DIACRITIC_ID) for d in diacs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T16:00:41.449125Z",
     "iopub.status.busy": "2025-11-26T16:00:41.448813Z",
     "iopub.status.idle": "2025-11-26T16:00:41.520970Z",
     "shell.execute_reply": "2025-11-26T16:00:41.520355Z",
     "shell.execute_reply.started": "2025-11-26T16:00:41.449066Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_val shape: (12472, 568)\n",
      "y_val shape: (12472, 568)\n"
     ]
    }
   ],
   "source": [
    "# Pad validation sequences\n",
    "x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val_raw, padding='post', value=0)\n",
    "y_val = tf.keras.preprocessing.sequence.pad_sequences(y_val_raw, padding='post', value=PAD_DIACRITIC_ID)\n",
    "\n",
    "print(f\"x_val shape: {x_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T16:00:42.981051Z",
     "iopub.status.busy": "2025-11-26T16:00:42.980375Z",
     "iopub.status.idle": "2025-11-26T16:00:45.549444Z",
     "shell.execute_reply": "2025-11-26T16:00:45.548815Z",
     "shell.execute_reply.started": "2025-11-26T16:00:42.981027Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 160ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_der_by_position(x_val, y_true, y_pred, char_to_id):\n",
    "    \"\"\"\n",
    "    Calculate DER separately for last characters and non-last characters in words\n",
    "    \n",
    "    Args:\n",
    "        x_val: Character sequences (samples × sequence_length)\n",
    "        y_true: Ground truth diacritic labels (samples × sequence_length)\n",
    "        y_pred: Predicted diacritic labels (samples × sequence_length)\n",
    "        char_to_id: Dictionary mapping characters to IDs\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (DER_non_last, DER_last, overall_DER)\n",
    "    \"\"\"\n",
    "    # Get space character ID\n",
    "    space_id = char_to_id.get(' ', char_to_id.get('UNK'))\n",
    "    pad_id = char_to_id.get('<PAD>', 0)\n",
    "    \n",
    "    non_last_errors = 0\n",
    "    non_last_total = 0\n",
    "    last_errors = 0\n",
    "    last_total = 0\n",
    "    \n",
    "    # Process each sequence\n",
    "    for char_seq, y_true_seq, y_pred_seq in zip(x_val, y_true, y_pred):\n",
    "        # Find valid (non-padding) characters\n",
    "        valid_mask = char_seq != pad_id\n",
    "        valid_indices = np.where(valid_mask)[0]\n",
    "        \n",
    "        if len(valid_indices) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Identify word boundaries (spaces and end of sequence)\n",
    "        i = 0\n",
    "        while i < len(valid_indices):\n",
    "            idx = valid_indices[i]\n",
    "            \n",
    "            # Skip spaces\n",
    "            if char_seq[idx] == space_id:\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            # Find the end of current word\n",
    "            word_start = i\n",
    "            while i < len(valid_indices) and char_seq[valid_indices[i]] != space_id:\n",
    "                i += 1\n",
    "            word_end = i - 1\n",
    "            \n",
    "            # Mark positions in the word\n",
    "            for j in range(word_start, word_end + 1):\n",
    "                pos_idx = valid_indices[j]\n",
    "                \n",
    "                # Skip if this position has padding in y_true\n",
    "                if y_true_seq[pos_idx] == PAD_DIACRITIC_ID:\n",
    "                    continue\n",
    "                \n",
    "                is_correct = (y_true_seq[pos_idx] == y_pred_seq[pos_idx])\n",
    "                \n",
    "                # Last character in word\n",
    "                if j == word_end:\n",
    "                    last_total += 1\n",
    "                    if not is_correct:\n",
    "                        last_errors += 1\n",
    "                # Non-last character in word\n",
    "                else:\n",
    "                    non_last_total += 1\n",
    "                    if not is_correct:\n",
    "                        non_last_errors += 1\n",
    "    \n",
    "    # Calculate DER for each category\n",
    "    der_non_last = (non_last_errors / non_last_total * 100) if non_last_total > 0 else 0\n",
    "    der_last = (last_errors / last_total * 100) if last_total > 0 else 0\n",
    "    \n",
    "    total_errors = non_last_errors + last_errors\n",
    "    total_chars = non_last_total + last_total\n",
    "    der_overall = (total_errors / total_chars * 100) if total_chars > 0 else 0\n",
    "    \n",
    "    return der_non_last, der_last, der_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T16:00:46.669238Z",
     "iopub.status.busy": "2025-11-26T16:00:46.668617Z",
     "iopub.status.idle": "2025-11-26T16:00:46.799396Z",
     "shell.execute_reply": "2025-11-26T16:00:46.798764Z",
     "shell.execute_reply.started": "2025-11-26T16:00:46.669213Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9955\n",
      "Diacritic Error Rate (DER): 10.47%\n",
      "Acutual Accuracy: 89.53%\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(y_pred, axis=-1)\n",
    "y_true = y_val\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true.flatten(), y_pred_classes.flatten())\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate DER by character position in words\n",
    "der_non_last, der_last, der_overall = calculate_der_by_position(x_val, y_true, y_pred_classes, char_to_id)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DER Analysis by Character Position in Words\")\n",
    "print(\"=\"*60)\n",
    "print(f\"DER for non-last characters: {der_non_last:.2f}%\")\n",
    "print(f\"DER for last characters:     {der_last:.2f}%\")\n",
    "print(f\"Overall DER:                 {der_overall:.2f}%\")\n",
    "print(f\"\\nAccuracy for non-last characters: {100 - der_non_last:.2f}%\")\n",
    "print(f\"Accuracy for last characters:     {100 - der_last:.2f}%\")\n",
    "print(f\"Acutual Accuracy: {100 - der_overall:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T16:17:09.024013Z",
     "iopub.status.busy": "2025-11-26T16:17:09.023262Z",
     "iopub.status.idle": "2025-11-26T16:17:09.031404Z",
     "shell.execute_reply": "2025-11-26T16:17:09.030352Z",
     "shell.execute_reply.started": "2025-11-26T16:17:09.023978Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def merge(x_test, y_pred_classes, char_to_id, diacritics_to_id):\n",
    "    \"\"\"\n",
    "    Merge character sequences with predicted diacritics to reconstruct text\n",
    "    \n",
    "    Args:\n",
    "        x_test: numpy array of character IDs (samples × sequence_length)\n",
    "        y_pred_classes: numpy array of predicted diacritic IDs (samples × sequence_length)\n",
    "        char_to_id: dictionary mapping characters to IDs\n",
    "        diacritics_to_id: dictionary mapping diacritics to IDs\n",
    "    \n",
    "    Returns:\n",
    "        List of reconstructed diacritized text strings\n",
    "    \"\"\"\n",
    "    # Create reverse mappings\n",
    "    id_to_char = {v: k for k, v in char_to_id.items()}\n",
    "    id_to_diacritic = {v: k for k, v in diacritics_to_id.items()}\n",
    "    \n",
    "    reconstructed_texts = []\n",
    "    \n",
    "    # Process each sample\n",
    "    for char_seq, diac_seq in zip(x_test, y_pred_classes):\n",
    "        text = \"\"\n",
    "        \n",
    "        for char_id, diac_id in zip(char_seq, diac_seq):\n",
    "            # Skip padding\n",
    "            if char_id == 0:  # PAD character\n",
    "                break\n",
    "            \n",
    "            # Get character\n",
    "            char = id_to_char.get(char_id, '')\n",
    "            \n",
    "            # Get diacritic\n",
    "            diacritic = id_to_diacritic.get(diac_id, '')\n",
    "            \n",
    "            # Combine character with diacritic\n",
    "            text += char + diacritic\n",
    "        \n",
    "        text = text.replace(\"UNK\", \" \")\n",
    "        reconstructed_texts.append(text)\n",
    "    \n",
    "    return reconstructed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T16:17:04.153979Z",
     "iopub.status.busy": "2025-11-26T16:17:04.153348Z",
     "iopub.status.idle": "2025-11-26T16:17:04.432609Z",
     "shell.execute_reply": "2025-11-26T16:17:04.431924Z",
     "shell.execute_reply.started": "2025-11-26T16:17:04.153949Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m test_sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mهذا نص تجريبي لاختبار نموذج تشكيل النص العربي.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m x_test \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m(test_sent)\n\u001b[0;32m      4\u001b[0m y_test_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39marray(x_test))\n\u001b[0;32m      5\u001b[0m y_test_pred_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_test_pred, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_model' is not defined"
     ]
    }
   ],
   "source": [
    "test_sent = \"هذا نص تجريبي لاختبار نموذج تشكيل النص العربي.\"\n",
    "\n",
    "x_test = test_model(test_sent)\n",
    "y_test_pred = model.predict(np.array(x_test))\n",
    "y_test_pred_classes = np.argmax(y_test_pred, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T16:17:05.834779Z",
     "iopub.status.busy": "2025-11-26T16:17:05.833959Z",
     "iopub.status.idle": "2025-11-26T16:17:05.839512Z",
     "shell.execute_reply": "2025-11-26T16:17:05.838830Z",
     "shell.execute_reply.started": "2025-11-26T16:17:05.834741Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 46, 15)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "هَذَا نَص تَجْرِيبِي لِاخْتِبَارِ نَمُوذَجِ تَشْكِيلِ النص الْعَرْبِي \n"
     ]
    }
   ],
   "source": [
    "output_sentences = merge(x_test, y_test_pred_classes, char_to_id, diacritics_to_id)[0]\n",
    "print(output_sentences)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
