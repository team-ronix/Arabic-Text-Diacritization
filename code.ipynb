{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:57:04.565619Z",
     "iopub.status.busy": "2025-11-29T02:57:04.565328Z",
     "iopub.status.idle": "2025-11-29T02:57:04.569384Z",
     "shell.execute_reply": "2025-11-29T02:57:04.568560Z",
     "shell.execute_reply.started": "2025-11-29T02:57:04.565598Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# GDRIVE_ID_DATA = \"1ONRQ36PFPnYNA4R6ZlmM7UQJ4LiAzEH0\"\n",
    "# !gdown $GDRIVE_ID_DATA -O Arabic-Text-Diacritization.zip\n",
    "# !unzip Arabic-Text-Diacritization.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:57:04.570673Z",
     "iopub.status.busy": "2025-11-29T02:57:04.570481Z",
     "iopub.status.idle": "2025-11-29T02:57:04.583704Z",
     "shell.execute_reply": "2025-11-29T02:57:04.582940Z",
     "shell.execute_reply.started": "2025-11-29T02:57:04.570658Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# GDRIVE_ID_LSTM_MODEL = \"1kLRQ3o7m57qK1OJOTA-K9OBYL29zuXjo\"\n",
    "# !gdown $GDRIVE_ID_LSTM_MODEL -O LSTM.joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:57:04.584531Z",
     "iopub.status.busy": "2025-11-29T02:57:04.584361Z",
     "iopub.status.idle": "2025-11-29T02:57:04.596933Z",
     "shell.execute_reply": "2025-11-29T02:57:04.596370Z",
     "shell.execute_reply.started": "2025-11-29T02:57:04.584518Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import tarfile\n",
    "\n",
    "# file_path = \"/kaggle/input/tashkeela/Tashkeela-arabic-diacritized-text-utf8-0.3.tar.bz2\"\n",
    "# extract_path = \"/kaggle/working/tashkeela_extracted\"\n",
    "\n",
    "# # Extract tar.bz2 file\n",
    "# with tarfile.open(file_path, \"r:bz2\") as tar:\n",
    "#     tar.extractall(path=extract_path)\n",
    "\n",
    "# extract_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Arabic letters and diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:57:04.598433Z",
     "iopub.status.busy": "2025-11-29T02:57:04.598270Z",
     "iopub.status.idle": "2025-11-29T02:57:04.610176Z",
     "shell.execute_reply": "2025-11-29T02:57:04.609548Z",
     "shell.execute_reply.started": "2025-11-29T02:57:04.598421Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import pyarabic.araby as araby\n",
    "import pyarabic.number as number\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import unicodedata\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input, Model\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:57:04.611026Z",
     "iopub.status.busy": "2025-11-29T02:57:04.610788Z",
     "iopub.status.idle": "2025-11-29T02:57:04.623497Z",
     "shell.execute_reply": "2025-11-29T02:57:04.622896Z",
     "shell.execute_reply.started": "2025-11-29T02:57:04.611010Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "window_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Configuration for Kaggle\n",
    "Check GPU availability and configure TensorFlow to use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:57:04.624341Z",
     "iopub.status.busy": "2025-11-29T02:57:04.624166Z",
     "iopub.status.idle": "2025-11-29T02:57:04.637636Z",
     "shell.execute_reply": "2025-11-29T02:57:04.636895Z",
     "shell.execute_reply.started": "2025-11-29T02:57:04.624328Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Physical devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Get GPU details\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth to avoid OOM errors\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        print(f\"GPU detected: {gpus}\")\n",
    "        print(f\"GPU Name: {tf.test.gpu_device_name()}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found. Training will use CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dictionaries and Create Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:57:04.638915Z",
     "iopub.status.busy": "2025-11-29T02:57:04.638396Z",
     "iopub.status.idle": "2025-11-29T02:57:04.655513Z",
     "shell.execute_reply": "2025-11-29T02:57:04.654854Z",
     "shell.execute_reply.started": "2025-11-29T02:57:04.638899Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "arabic_letters = []\n",
    "diacritics = []\n",
    "diacritics_to_id = {}\n",
    "with open('./utils/arabic_letters.pickle', 'rb') as f:\n",
    "    arabic_letters = pickle.load(f)\n",
    "with open('./utils/diacritics.pickle', 'rb') as f:\n",
    "    diacritics = pickle.load(f)\n",
    "with open('./utils/diacritic2id.pickle', 'rb') as f:\n",
    "    diacritics_to_id = pickle.load(f)\n",
    "\n",
    "arabic_letters_sorted = sorted(arabic_letters)\n",
    "char_to_id = {char: idx + 1 for idx, char in enumerate(arabic_letters_sorted)}\n",
    "char_to_id['<PAD>'] = 0\n",
    "char_to_id['UNK'] = len(char_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:57:04.656397Z",
     "iopub.status.busy": "2025-11-29T02:57:04.656168Z",
     "iopub.status.idle": "2025-11-29T02:57:04.676308Z",
     "shell.execute_reply": "2025-11-29T02:57:04.675660Z",
     "shell.execute_reply.started": "2025-11-29T02:57:04.656377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Build word vocabulary from training data\n",
    "def build_word_vocabulary(data):\n",
    "    \"\"\"\n",
    "    Build word vocabulary from training data\n",
    "    \n",
    "    Args:\n",
    "        data: List of text samples\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping words to IDs\n",
    "    \"\"\"\n",
    "    word_counts = {}\n",
    "    for text in data:\n",
    "        # Remove diacritics for word tokenization\n",
    "        text_no_diac = araby.strip_diacritics(text)\n",
    "        words = araby.tokenize(text_no_diac)\n",
    "        for word in words:\n",
    "            if word.strip():  # Skip empty strings\n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    \n",
    "    # Sort by frequency and create word_to_id mapping\n",
    "    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    word_to_id = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for idx, (word, _) in enumerate(sorted_words):\n",
    "        word_to_id[word] = idx + 2\n",
    "    \n",
    "    return word_to_id\n",
    "\n",
    "# Will be populated after loading training data\n",
    "word_to_id = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Read train and val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:57:04.677160Z",
     "iopub.status.busy": "2025-11-29T02:57:04.676909Z",
     "iopub.status.idle": "2025-11-29T02:57:04.866301Z",
     "shell.execute_reply": "2025-11-29T02:57:04.865695Z",
     "shell.execute_reply.started": "2025-11-29T02:57:04.677140Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "val_data = []\n",
    "with open('./data/train.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        train_data.append(line.strip())\n",
    "with open('./data/val.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        val_data.append(line.strip())\n",
    "print(len(train_data))\n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:57:04.868815Z",
     "iopub.status.busy": "2025-11-29T02:57:04.868601Z",
     "iopub.status.idle": "2025-11-29T02:57:04.875521Z",
     "shell.execute_reply": "2025-11-29T02:57:04.874803Z",
     "shell.execute_reply.started": "2025-11-29T02:57:04.868798Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_arabic_text(text):\n",
    "    \"\"\"\n",
    "    Clean text to keep only Arabic letters, diacritics, and spaces\n",
    "    \"\"\"\n",
    "    # Create a set of allowed characters (Arabic letters + diacritics + space)\n",
    "    allowed_chars = arabic_letters.union(diacritics, {' ', '\\t', '\\n'})\n",
    "    \n",
    "    # Filter the text to keep only allowed characters\n",
    "    cleaned_text = ''.join(char for char in text if char in allowed_chars)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def split_sentences(sentences, window_size=window_size):\n",
    "    all_segments = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = araby.tokenize(sentence)\n",
    "        current_segment = []\n",
    "        current_len = 0\n",
    "        \n",
    "        for word in words:\n",
    "            word_len = len(word)\n",
    "            add_space = 1 if current_segment else 0\n",
    "            \n",
    "            if current_len + word_len + add_space <= window_size:\n",
    "                current_segment.append(word)\n",
    "                current_len += word_len + add_space\n",
    "            else:\n",
    "                # save the segment\n",
    "                if current_segment:\n",
    "                    all_segments.append(\" \".join(current_segment))\n",
    "                \n",
    "                # start new segment\n",
    "                current_segment = [word]\n",
    "                current_len = word_len\n",
    "        \n",
    "        # append the final segment of the sentence\n",
    "        if current_segment:\n",
    "            all_segments.append(\" \".join(current_segment))\n",
    "\n",
    "    return all_segments\n",
    "\n",
    "\n",
    "\n",
    "def sentence_tokeniz(sentences):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        subsentences = araby.sentence_tokenize(sentence)\n",
    "        tokenized_sentences.extend(subsentences)\n",
    "    return tokenized_sentences\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:57:04.876411Z",
     "iopub.status.busy": "2025-11-29T02:57:04.876229Z",
     "iopub.status.idle": "2025-11-29T02:57:17.019136Z",
     "shell.execute_reply": "2025-11-29T02:57:17.018445Z",
     "shell.execute_reply.started": "2025-11-29T02:57:04.876398Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building word vocabulary...\n",
      "Word vocabulary size: 105864\n"
     ]
    }
   ],
   "source": [
    "train_data = sentence_tokeniz(train_data)\n",
    "val_data = sentence_tokeniz(val_data)\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i] = clean_arabic_text(train_data[i])\n",
    "for i in range(len(val_data)):\n",
    "    val_data[i] = clean_arabic_text(val_data[i])\n",
    "\n",
    "train_data = split_sentences(train_data, window_size)\n",
    "val_data = split_sentences(val_data, window_size)\n",
    "\n",
    "# Build word vocabulary from training data\n",
    "print(\"Building word vocabulary...\")\n",
    "word_to_id = build_word_vocabulary(train_data)\n",
    "print(f\"Word vocabulary size: {len(word_to_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:57:17.019920Z",
     "iopub.status.busy": "2025-11-29T02:57:17.019738Z",
     "iopub.status.idle": "2025-11-29T02:57:17.024925Z",
     "shell.execute_reply": "2025-11-29T02:57:17.024310Z",
     "shell.execute_reply.started": "2025-11-29T02:57:17.019905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def is_diacritic(ch):\n",
    "    # Unicode combining marks (Arabic diacritics are combining marks)\n",
    "    return unicodedata.combining(ch) != 0\n",
    "\n",
    "def extract_base_and_diacritics(text):\n",
    "    # normalize to NFC so base+combining marks are consistent\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    bases = []\n",
    "    diacs = []\n",
    "    current_base = None\n",
    "    current_diac = ''\n",
    "    for ch in text:\n",
    "        if is_diacritic(ch):\n",
    "            # accumulate diacritics for current base\n",
    "            current_diac += ch\n",
    "        else:\n",
    "            # new base character\n",
    "            if current_base is not None:\n",
    "                bases.append(current_base)\n",
    "                diacs.append(current_diac)\n",
    "            current_base = ch\n",
    "            current_diac = ''\n",
    "    # append last\n",
    "    if current_base is not None:\n",
    "        bases.append(current_base)\n",
    "        diacs.append(current_diac)\n",
    "    return bases, diacs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:57:17.025932Z",
     "iopub.status.busy": "2025-11-29T02:57:17.025708Z",
     "iopub.status.idle": "2025-11-29T02:57:17.041337Z",
     "shell.execute_reply": "2025-11-29T02:57:17.040681Z",
     "shell.execute_reply.started": "2025-11-29T02:57:17.025916Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_char_and_word_features(text, word_to_id):\n",
    "    \"\"\"\n",
    "    Extract both character-level and word-level features from text\n",
    "    \n",
    "    Args:\n",
    "        text: Input text with diacritics\n",
    "        word_to_id: Dictionary mapping words to IDs\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (char_ids, diacritic_ids, word_ids, word_positions)\n",
    "        - char_ids: List of character IDs\n",
    "        - diacritic_ids: List of diacritic IDs for each character\n",
    "        - word_ids: List of word IDs aligned with characters\n",
    "        - word_positions: List indicating position in word (0=start, 1=middle, 2=end, 3=single-char word)\n",
    "    \"\"\"\n",
    "    # Extract base characters and diacritics\n",
    "    bases, diacs = extract_base_and_diacritics(text)\n",
    "    \n",
    "    UNKNOWN_DIACRITIC_ID = diacritics_to_id.get('', len(diacritics_to_id) - 1)\n",
    "\n",
    "    # Convert characters to IDs\n",
    "    char_ids = [char_to_id.get(c, char_to_id['UNK']) for c in bases]\n",
    "    diacritic_ids = [diacritics_to_id.get(d, UNKNOWN_DIACRITIC_ID) for d in diacs]\n",
    "    \n",
    "    # Extract word-level features\n",
    "    text_no_diac = araby.strip_diacritics(text)\n",
    "    words = araby.tokenize(text_no_diac)\n",
    "    \n",
    "    # Create word ID sequence aligned with characters\n",
    "    word_ids = []\n",
    "    word_positions = []  # 0=start, 1=middle, 2=end, 3=single-char, 4=space\n",
    "    \n",
    "    char_idx = 0\n",
    "    for word in words:\n",
    "        if not word.strip():\n",
    "            continue\n",
    "        \n",
    "        word_id = word_to_id.get(word, word_to_id['<UNK>'])\n",
    "        word_len = len(word)\n",
    "        \n",
    "        # Assign same word ID to all characters in the word\n",
    "        for i in range(word_len):\n",
    "            if char_idx < len(char_ids):\n",
    "                word_ids.append(word_id)\n",
    "                \n",
    "                # Determine position in word\n",
    "                if word_len == 1:\n",
    "                    word_positions.append(3)  # Single character word\n",
    "                elif i == 0:\n",
    "                    word_positions.append(0)  # Start of word\n",
    "                elif i == word_len - 1:\n",
    "                    word_positions.append(2)  # End of word\n",
    "                else:\n",
    "                    word_positions.append(1)  # Middle of word\n",
    "                \n",
    "                char_idx += 1\n",
    "        \n",
    "        # Handle space after word\n",
    "        if char_idx < len(char_ids) and bases[char_idx] == ' ':\n",
    "            word_ids.append(0)  # PAD for space\n",
    "            word_positions.append(4)  # Space position\n",
    "            char_idx += 1\n",
    "    \n",
    "    # Fill remaining with PAD\n",
    "    while len(word_ids) < len(char_ids):\n",
    "        word_ids.append(0)\n",
    "        word_positions.append(4)  # Treat padding as space\n",
    "    \n",
    "    return char_ids, diacritic_ids, word_ids, word_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prepare data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:57:17.042240Z",
     "iopub.status.busy": "2025-11-29T02:57:17.042049Z",
     "iopub.status.idle": "2025-11-29T02:57:30.849533Z",
     "shell.execute_reply": "2025-11-29T02:57:30.848680Z",
     "shell.execute_reply.started": "2025-11-29T02:57:17.042224Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting character and word-level features from training data...\n",
      "Training samples: 172467\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data - extract both character and word level features\n",
    "x_train_char_raw = []\n",
    "y_train_raw = []\n",
    "x_train_word_raw = []\n",
    "x_train_word_position_raw = []\n",
    "\n",
    "# Use a constant for unknown diacritic instead of hardcoded value\n",
    "UNKNOWN_DIACRITIC_ID = diacritics_to_id.get('', len(diacritics_to_id) - 1)\n",
    "\n",
    "print(\"Extracting character and word-level features from training data...\")\n",
    "for text in train_data:\n",
    "    char_ids, diacritic_ids, word_ids, word_positions = extract_char_and_word_features(text, word_to_id)\n",
    "    \n",
    "    x_train_char_raw.append(char_ids)\n",
    "    y_train_raw.append(diacritic_ids)\n",
    "    x_train_word_raw.append(word_ids)\n",
    "    x_train_word_position_raw.append(word_positions)\n",
    "\n",
    "print(f\"Training samples: {len(x_train_char_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:57:30.850791Z",
     "iopub.status.busy": "2025-11-29T02:57:30.850530Z",
     "iopub.status.idle": "2025-11-29T02:57:30.974527Z",
     "shell.execute_reply": "2025-11-29T02:57:30.973828Z",
     "shell.execute_reply.started": "2025-11-29T02:57:30.850768Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n",
      "Position encoding: 0=start, 1=middle, 2=end, 3=single-char, 4=space\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ char_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_input      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ char_embedding      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,864</span> │ char_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_embedding      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">13,550,592</span> │ word_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_embedding  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │ position_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ feature_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ char_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ word_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ position_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_16        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ char_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bilstm              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,083,392</span> │ feature_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ diacritic_output    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,695</span> │ bilstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ char_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_input      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ char_embedding      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │      \u001b[38;5;34m4,864\u001b[0m │ char_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_embedding      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │ \u001b[38;5;34m13,550,592\u001b[0m │ word_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_embedding  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)  │         \u001b[38;5;34m80\u001b[0m │ position_input[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ feature_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m272\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ char_embedding[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ word_embedding[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ position_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_16        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ char_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bilstm              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │  \u001b[38;5;34m1,083,392\u001b[0m │ feature_concat[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ diacritic_output    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)  │      \u001b[38;5;34m7,695\u001b[0m │ bilstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,646,623</span> (55.87 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,646,623\u001b[0m (55.87 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,646,623</span> (55.87 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,646,623\u001b[0m (55.87 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vocab sizes (your variables)\n",
    "vocab_size = len(char_to_id)\n",
    "num_diacritics = len(diacritics_to_id)\n",
    "word_vocab_size = len(word_to_id)\n",
    "\n",
    "# ==============================\n",
    "# 1. CHARACTER INPUT\n",
    "# ==============================\n",
    "char_input = Input(shape=(None,), name='char_input')\n",
    "char_embedding = layers.Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=128,\n",
    "    mask_zero=True,\n",
    "    name='char_embedding'\n",
    ")(char_input)\n",
    "\n",
    "# ==============================\n",
    "# 2. WORD INPUT\n",
    "# ==============================\n",
    "word_input = Input(shape=(None,), name='word_input')\n",
    "word_embedding = layers.Embedding(\n",
    "    input_dim=word_vocab_size,\n",
    "    output_dim=128,\n",
    "    mask_zero=True,\n",
    "    name='word_embedding'\n",
    ")(word_input)\n",
    "\n",
    "# ==============================\n",
    "# 3. WORD POSITION INPUT (0=start, 1=middle, 2=end, 3=single-char, 4=space)\n",
    "# ==============================\n",
    "position_input = Input(shape=(None,), name='position_input')\n",
    "position_embedding = layers.Embedding(\n",
    "    input_dim=5,  # 5 position types\n",
    "    output_dim=16,\n",
    "    mask_zero=False,\n",
    "    name='position_embedding'\n",
    ")(position_input)\n",
    "\n",
    "# ==============================\n",
    "# 4. CONCATENATE FEATURES\n",
    "# ==============================\n",
    "combined = layers.Concatenate(name='feature_concat')([\n",
    "    char_embedding,\n",
    "    word_embedding,\n",
    "    position_embedding\n",
    "])\n",
    "\n",
    "# IMPORTANT FIX: Use only char mask to avoid GPU mask mismatch\n",
    "combined._keras_mask = char_embedding._keras_mask\n",
    "\n",
    "# ==============================\n",
    "# 5. BiLSTM FOR DIACRITIZATION\n",
    "# ==============================\n",
    "lstm_out = layers.Bidirectional(\n",
    "    layers.LSTM(\n",
    "        256,\n",
    "        return_sequences=True,\n",
    "        activation='tanh',\n",
    "        recurrent_activation='sigmoid'\n",
    "    ),\n",
    "    name='bilstm'\n",
    ")(combined)\n",
    "\n",
    "# ==============================\n",
    "# 6. OUTPUT LAYER\n",
    "# ==============================\n",
    "output = layers.Dense(\n",
    "    num_diacritics,\n",
    "    activation='softmax',\n",
    "    name='diacritic_output'\n",
    ")(lstm_out)\n",
    "\n",
    "# ==============================\n",
    "# 7. CREATE & COMPILE MODEL\n",
    "# ==============================\n",
    "model = Model(\n",
    "    inputs=[char_input, word_input, position_input],\n",
    "    outputs=output\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(\"Position encoding: 0=start, 1=middle, 2=end, 3=single-char, 4=space\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:57:30.975570Z",
     "iopub.status.busy": "2025-11-29T02:57:30.975329Z",
     "iopub.status.idle": "2025-11-29T02:57:34.680093Z",
     "shell.execute_reply": "2025-11-29T02:57:34.679441Z",
     "shell.execute_reply.started": "2025-11-29T02:57:30.975554Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_char shape: (172467, 607)\n",
      "x_train_word shape: (172467, 607)\n",
      "x_train_position shape: (172467, 607)\n",
      "y_train shape: (172467, 607)\n"
     ]
    }
   ],
   "source": [
    "# Pad sequences to same length\n",
    "PAD_DIACRITIC_ID = diacritics_to_id.get('', 0)  # Use empty string diacritic for padding\n",
    "\n",
    "x_train_char = tf.keras.preprocessing.sequence.pad_sequences(x_train_char_raw, padding='post', value=0)\n",
    "x_train_word = tf.keras.preprocessing.sequence.pad_sequences(x_train_word_raw, padding='post', value=0)\n",
    "x_train_position = tf.keras.preprocessing.sequence.pad_sequences(x_train_word_position_raw, padding='post', value=4)\n",
    "y_train = tf.keras.preprocessing.sequence.pad_sequences(y_train_raw, padding='post', value=PAD_DIACRITIC_ID)\n",
    "\n",
    "print(f\"x_train_char shape: {x_train_char.shape}\")\n",
    "print(f\"x_train_word shape: {x_train_word.shape}\")\n",
    "print(f\"x_train_position shape: {x_train_position.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:57:34.680999Z",
     "iopub.status.busy": "2025-11-29T02:57:34.680737Z",
     "iopub.status.idle": "2025-11-29T03:20:58.418027Z",
     "shell.execute_reply": "2025-11-29T03:20:58.417426Z",
     "shell.execute_reply.started": "2025-11-29T02:57:34.680977Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 104ms/step - accuracy: 0.9784 - loss: 1.9717\n",
      "Epoch 2/10\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 104ms/step - accuracy: 0.9974 - loss: 0.5436\n",
      "Epoch 3/10\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 104ms/step - accuracy: 0.9980 - loss: 0.1661\n",
      "Epoch 4/10\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 104ms/step - accuracy: 0.9983 - loss: 0.0683\n",
      "Epoch 5/10\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 104ms/step - accuracy: 0.9985 - loss: 0.0332\n",
      "Epoch 6/10\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 103ms/step - accuracy: 0.9988 - loss: 0.0178\n",
      "Epoch 7/10\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 104ms/step - accuracy: 0.9990 - loss: 0.0103\n",
      "Epoch 8/10\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 104ms/step - accuracy: 0.9991 - loss: 0.0063\n",
      "Epoch 9/10\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 104ms/step - accuracy: 0.9993 - loss: 0.0041\n",
      "Epoch 10/10\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 104ms/step - accuracy: 0.9994 - loss: 0.0028\n"
     ]
    }
   ],
   "source": [
    "# # Train model with multi-input (character + word features)\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(\n",
    "        {'char_input': x_train_char, 'word_input': x_train_word, 'position_input': x_train_position},\n",
    "        y_train,\n",
    "        epochs=10,\n",
    "        batch_size=128,\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:20:58.419571Z",
     "iopub.status.busy": "2025-11-29T03:20:58.419376Z",
     "iopub.status.idle": "2025-11-29T03:20:58.422881Z",
     "shell.execute_reply": "2025-11-29T03:20:58.422196Z",
     "shell.execute_reply.started": "2025-11-29T03:20:58.419556Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# joblib.dump(model, \"/kaggle/working/LSTM2.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:20:58.424901Z",
     "iopub.status.busy": "2025-11-29T03:20:58.424355Z",
     "iopub.status.idle": "2025-11-29T03:20:58.437171Z",
     "shell.execute_reply": "2025-11-29T03:20:58.436401Z",
     "shell.execute_reply.started": "2025-11-29T03:20:58.424882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model = joblib.load(\"./models/LSTM.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:20:58.438133Z",
     "iopub.status.busy": "2025-11-29T03:20:58.437852Z",
     "iopub.status.idle": "2025-11-29T03:20:59.083284Z",
     "shell.execute_reply": "2025-11-29T03:20:59.082514Z",
     "shell.execute_reply.started": "2025-11-29T03:20:58.438117Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting character and word-level features from validation data...\n",
      "Validation samples: 8332\n"
     ]
    }
   ],
   "source": [
    "# Prepare validation data - extract both character and word level features\n",
    "x_val_char_raw = []\n",
    "y_val_raw = []\n",
    "x_val_word_raw = []\n",
    "x_val_word_position_raw = []\n",
    "\n",
    "print(\"Extracting character and word-level features from validation data...\")\n",
    "for text in val_data:\n",
    "    char_ids, diacritic_ids, word_ids, word_positions = extract_char_and_word_features(text, word_to_id)\n",
    "    \n",
    "    x_val_char_raw.append(char_ids)\n",
    "    y_val_raw.append(diacritic_ids)\n",
    "    x_val_word_raw.append(word_ids)\n",
    "    x_val_word_position_raw.append(word_positions)\n",
    "\n",
    "print(f\"Validation samples: {len(x_val_char_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:20:59.084836Z",
     "iopub.status.busy": "2025-11-29T03:20:59.084076Z",
     "iopub.status.idle": "2025-11-29T03:20:59.264417Z",
     "shell.execute_reply": "2025-11-29T03:20:59.263842Z",
     "shell.execute_reply.started": "2025-11-29T03:20:59.084816Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_val_char shape: (8332, 597)\n",
      "x_val_word shape: (8332, 597)\n",
      "x_val_position shape: (8332, 597)\n",
      "y_val shape: (8332, 597)\n"
     ]
    }
   ],
   "source": [
    "# Pad validation sequences\n",
    "x_val_char = tf.keras.preprocessing.sequence.pad_sequences(x_val_char_raw, padding='post', value=0)\n",
    "x_val_word = tf.keras.preprocessing.sequence.pad_sequences(x_val_word_raw, padding='post', value=0)\n",
    "x_val_position = tf.keras.preprocessing.sequence.pad_sequences(x_val_word_position_raw, padding='post', value=4)\n",
    "y_val = tf.keras.preprocessing.sequence.pad_sequences(y_val_raw, padding='post', value=PAD_DIACRITIC_ID)\n",
    "\n",
    "print(f\"x_val_char shape: {x_val_char.shape}\")\n",
    "print(f\"x_val_word shape: {x_val_word.shape}\")\n",
    "print(f\"x_val_position shape: {x_val_position.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:20:59.267026Z",
     "iopub.status.busy": "2025-11-29T03:20:59.266804Z",
     "iopub.status.idle": "2025-11-29T03:21:04.699515Z",
     "shell.execute_reply": "2025-11-29T03:21:04.698896Z",
     "shell.execute_reply.started": "2025-11-29T03:20:59.267009Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict({'char_input': x_val_char, 'word_input': x_val_word, 'position_input': x_val_position})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:21:04.700904Z",
     "iopub.status.busy": "2025-11-29T03:21:04.700651Z",
     "iopub.status.idle": "2025-11-29T03:21:04.709400Z",
     "shell.execute_reply": "2025-11-29T03:21:04.708816Z",
     "shell.execute_reply.started": "2025-11-29T03:21:04.700882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_der_by_position(x_val_char, y_true, y_pred, char_to_id):\n",
    "    \"\"\"\n",
    "    Calculate DER separately for last characters and non-last characters in words\n",
    "    \n",
    "    Args:\n",
    "        x_val_char: Character sequences (samples × sequence_length)\n",
    "        y_true: Ground truth diacritic labels (samples × sequence_length)\n",
    "        y_pred: Predicted diacritic labels (samples × sequence_length)\n",
    "        char_to_id: Dictionary mapping characters to IDs\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (DER_non_last, DER_last, overall_DER)\n",
    "    \"\"\"\n",
    "    # Get space character ID\n",
    "    space_id = char_to_id.get(' ', char_to_id.get('UNK'))\n",
    "    pad_id = char_to_id.get('<PAD>', 0)\n",
    "    \n",
    "    non_last_errors = 0\n",
    "    non_last_total = 0\n",
    "    last_errors = 0\n",
    "    last_total = 0\n",
    "    \n",
    "    # Process each sequence\n",
    "    for char_seq, y_true_seq, y_pred_seq in zip(x_val_char, y_true, y_pred):\n",
    "        # Find valid (non-padding) characters\n",
    "        valid_mask = char_seq != pad_id\n",
    "        valid_indices = np.where(valid_mask)[0]\n",
    "        \n",
    "        if len(valid_indices) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Identify word boundaries (spaces and end of sequence)\n",
    "        i = 0\n",
    "        while i < len(valid_indices):\n",
    "            idx = valid_indices[i]\n",
    "            \n",
    "            # Skip spaces\n",
    "            if char_seq[idx] == space_id:\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            # Find the end of current word\n",
    "            word_start = i\n",
    "            while i < len(valid_indices) and char_seq[valid_indices[i]] != space_id:\n",
    "                i += 1\n",
    "            word_end = i - 1\n",
    "            \n",
    "            # Mark positions in the word\n",
    "            for j in range(word_start, word_end + 1):\n",
    "                pos_idx = valid_indices[j]\n",
    "                \n",
    "                # Skip if this position has padding in y_true\n",
    "                if y_true_seq[pos_idx] == PAD_DIACRITIC_ID:\n",
    "                    continue\n",
    "                \n",
    "                is_correct = (y_true_seq[pos_idx] == y_pred_seq[pos_idx])\n",
    "                \n",
    "                # Last character in word\n",
    "                if j == word_end:\n",
    "                    last_total += 1\n",
    "                    if not is_correct:\n",
    "                        last_errors += 1\n",
    "                # Non-last character in word\n",
    "                else:\n",
    "                    non_last_total += 1\n",
    "                    if not is_correct:\n",
    "                        non_last_errors += 1\n",
    "    \n",
    "    # Calculate DER for each category\n",
    "    der_non_last = (non_last_errors / non_last_total * 100) if non_last_total > 0 else 0\n",
    "    der_last = (last_errors / last_total * 100) if last_total > 0 else 0\n",
    "    \n",
    "    total_errors = non_last_errors + last_errors\n",
    "    total_chars = non_last_total + last_total\n",
    "    der_overall = (total_errors / total_chars * 100) if total_chars > 0 else 0\n",
    "    \n",
    "    return der_non_last, der_last, der_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:21:04.710278Z",
     "iopub.status.busy": "2025-11-29T03:21:04.710078Z",
     "iopub.status.idle": "2025-11-29T03:21:04.940979Z",
     "shell.execute_reply": "2025-11-29T03:21:04.940224Z",
     "shell.execute_reply.started": "2025-11-29T03:21:04.710264Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9974\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(y_pred, axis=-1)\n",
    "y_true = y_val\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true.flatten(), y_pred_classes.flatten())\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:21:04.942223Z",
     "iopub.status.busy": "2025-11-29T03:21:04.941981Z",
     "iopub.status.idle": "2025-11-29T03:21:06.958315Z",
     "shell.execute_reply": "2025-11-29T03:21:06.957698Z",
     "shell.execute_reply.started": "2025-11-29T03:21:04.942204Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DER Analysis by Character Position in Words\n",
      "============================================================\n",
      "DER for non-last characters: 2.97%\n",
      "DER for last characters:     5.81%\n",
      "Overall DER:                 3.65%\n",
      "\n",
      "Accuracy for non-last characters: 97.03%\n",
      "Accuracy for last characters:     94.19%\n",
      "Acutual Accuracy: 96.35%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate DER by character position in words\n",
    "der_non_last, der_last, der_overall = calculate_der_by_position(x_val_char, y_true, y_pred_classes, char_to_id)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DER Analysis by Character Position in Words\")\n",
    "print(\"=\"*60)\n",
    "print(f\"DER for non-last characters: {der_non_last:.2f}%\")\n",
    "print(f\"DER for last characters:     {der_last:.2f}%\")\n",
    "print(f\"Overall DER:                 {der_overall:.2f}%\")\n",
    "print(f\"\\nAccuracy for non-last characters: {100 - der_non_last:.2f}%\")\n",
    "print(f\"Accuracy for last characters:     {100 - der_last:.2f}%\")\n",
    "print(f\"Acutual Accuracy: {100 - der_overall:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:21:06.959239Z",
     "iopub.status.busy": "2025-11-29T03:21:06.959006Z",
     "iopub.status.idle": "2025-11-29T03:21:06.964684Z",
     "shell.execute_reply": "2025-11-29T03:21:06.964116Z",
     "shell.execute_reply.started": "2025-11-29T03:21:06.959222Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def merge(x_test, y_pred_classes, char_to_id, diacritics_to_id):\n",
    "    \"\"\"\n",
    "    Merge character sequences with predicted diacritics to reconstruct text\n",
    "    \n",
    "    Args:\n",
    "        x_test: numpy array of character IDs (samples × sequence_length)\n",
    "        y_pred_classes: numpy array of predicted diacritic IDs (samples × sequence_length)\n",
    "        char_to_id: dictionary mapping characters to IDs\n",
    "        diacritics_to_id: dictionary mapping diacritics to IDs\n",
    "    \n",
    "    Returns:\n",
    "        List of reconstructed diacritized text strings\n",
    "    \"\"\"\n",
    "    # Create reverse mappings\n",
    "    id_to_char = {v: k for k, v in char_to_id.items()}\n",
    "    id_to_diacritic = {v: k for k, v in diacritics_to_id.items()}\n",
    "    \n",
    "    reconstructed_texts = []\n",
    "    \n",
    "    # Process each sample\n",
    "    for char_seq, diac_seq in zip(x_test, y_pred_classes):\n",
    "        text = \"\"\n",
    "        \n",
    "        for char_id, diac_id in zip(char_seq, diac_seq):\n",
    "            # Skip padding\n",
    "            if char_id == 0:  # PAD character\n",
    "                break\n",
    "            \n",
    "            # Get character\n",
    "            char = id_to_char.get(char_id, '')\n",
    "            \n",
    "            # Get diacritic\n",
    "            diacritic = id_to_diacritic.get(diac_id, '')\n",
    "            \n",
    "            # Combine character with diacritic\n",
    "            text += char + diacritic\n",
    "        \n",
    "        text = text.replace(\"UNK\", \" \")\n",
    "        reconstructed_texts.append(text)\n",
    "    \n",
    "    return reconstructed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:21:06.965732Z",
     "iopub.status.busy": "2025-11-29T03:21:06.965502Z",
     "iopub.status.idle": "2025-11-29T03:21:06.983570Z",
     "shell.execute_reply": "2025-11-29T03:21:06.982935Z",
     "shell.execute_reply.started": "2025-11-29T03:21:06.965713Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_model_with_word_features(text, word_to_id):\n",
    "    \"\"\"\n",
    "    Prepare test text for prediction with word-level features\n",
    "    \n",
    "    Args:\n",
    "        text: Input text without diacritics\n",
    "        word_to_id: Dictionary mapping words to IDs\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (char_ids, word_ids, word_positions) padded and ready for prediction\n",
    "    \"\"\"\n",
    "    # Strip diacritics if present\n",
    "    text_clean = araby.strip_diacritics(text)\n",
    "    \n",
    "    # Get characters\n",
    "    char_ids = [char_to_id.get(c, char_to_id['UNK']) for c in text_clean]\n",
    "    \n",
    "    # Get words\n",
    "    words = araby.tokenize(text_clean)\n",
    "    \n",
    "    # Create word ID sequence aligned with characters\n",
    "    word_ids = []\n",
    "    word_positions = []  # 0=start, 1=middle, 2=end, 3=single-char, 4=space\n",
    "    \n",
    "    char_idx = 0\n",
    "    for word in words:\n",
    "        if not word.strip():\n",
    "            continue\n",
    "        \n",
    "        word_id = word_to_id.get(word, word_to_id['<UNK>'])\n",
    "        word_len = len(word)\n",
    "        \n",
    "        for i in range(word_len):\n",
    "            if char_idx < len(char_ids):\n",
    "                word_ids.append(word_id)\n",
    "                \n",
    "                # Determine position in word\n",
    "                if word_len == 1:\n",
    "                    word_positions.append(3)  # Single character word\n",
    "                elif i == 0:\n",
    "                    word_positions.append(0)  # Start of word\n",
    "                elif i == word_len - 1:\n",
    "                    word_positions.append(2)  # End of word\n",
    "                else:\n",
    "                    word_positions.append(1)  # Middle of word\n",
    "                \n",
    "                char_idx += 1\n",
    "        \n",
    "        # Handle space\n",
    "        if char_idx < len(char_ids) and text_clean[char_idx] == ' ':\n",
    "            word_ids.append(0)\n",
    "            word_positions.append(4)  # Space position\n",
    "            char_idx += 1\n",
    "    \n",
    "    # Fill remaining\n",
    "    while len(word_ids) < len(char_ids):\n",
    "        word_ids.append(0)\n",
    "        word_positions.append(4)\n",
    "    \n",
    "    # Pad to model expected shape (add batch dimension)\n",
    "    char_ids_padded = tf.keras.preprocessing.sequence.pad_sequences([char_ids], padding='post', value=0)\n",
    "    word_ids_padded = tf.keras.preprocessing.sequence.pad_sequences([word_ids], padding='post', value=0)\n",
    "    positions_padded = tf.keras.preprocessing.sequence.pad_sequences([word_positions], padding='post', value=4)\n",
    "    \n",
    "    return char_ids_padded, word_ids_padded, positions_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:21:06.984491Z",
     "iopub.status.busy": "2025-11-29T03:21:06.984192Z",
     "iopub.status.idle": "2025-11-29T03:21:07.311439Z",
     "shell.execute_reply": "2025-11-29T03:21:07.310707Z",
     "shell.execute_reply.started": "2025-11-29T03:21:06.984475Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step\n"
     ]
    }
   ],
   "source": [
    "test_sent = \"هذا نص تجريبي لاختبار نموذج تشكيل النص العربي\"\n",
    "\n",
    "x_test_char, x_test_word, x_test_position = test_model_with_word_features(test_sent, word_to_id)\n",
    "y_test_pred = model.predict({\n",
    "    'char_input': x_test_char, \n",
    "    'word_input': x_test_word, \n",
    "    'position_input': x_test_position\n",
    "})\n",
    "y_test_pred_classes = np.argmax(y_test_pred, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:21:07.312929Z",
     "iopub.status.busy": "2025-11-29T03:21:07.312317Z",
     "iopub.status.idle": "2025-11-29T03:21:07.317147Z",
     "shell.execute_reply": "2025-11-29T03:21:07.316608Z",
     "shell.execute_reply.started": "2025-11-29T03:21:07.312909Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 45, 15)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:21:07.318159Z",
     "iopub.status.busy": "2025-11-29T03:21:07.317897Z",
     "iopub.status.idle": "2025-11-29T03:21:07.331240Z",
     "shell.execute_reply": "2025-11-29T03:21:07.330662Z",
     "shell.execute_reply.started": "2025-11-29T03:21:07.318135Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: هذا نص تجريبي لاختبار نموذج تشكيل النص العربي\n",
      "Diacritized output: هَذَا نَص تَجْرِيبِي لِاخْتِبَارٍ نَمُوذِجٍ تَشْكِيلِ النص الْعَرَبِي\n"
     ]
    }
   ],
   "source": [
    "output_sentences = merge(x_test_char, y_test_pred_classes, char_to_id, diacritics_to_id)[0]\n",
    "print(\"Input text:\", test_sent)\n",
    "print(\"Diacritized output:\", output_sentences)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
