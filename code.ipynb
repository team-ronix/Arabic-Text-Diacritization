{"metadata":{"kernelspec":{"display_name":"IP2","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10978,"sourceType":"datasetVersion","datasetId":7757}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Download data and models","metadata":{}},{"cell_type":"code","source":"# GDRIVE_ID_DATA = \"1ONRQ36PFPnYNA4R6ZlmM7UQJ4LiAzEH0\"\n# !gdown $GDRIVE_ID_DATA -O Arabic-Text-Diacritization.zip\n# !unzip Arabic-Text-Diacritization.zip","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:42:49.115070Z","iopub.execute_input":"2025-11-29T04:42:49.115359Z","iopub.status.idle":"2025-11-29T04:42:49.119020Z","shell.execute_reply.started":"2025-11-29T04:42:49.115340Z","shell.execute_reply":"2025-11-29T04:42:49.118272Z"},"trusted":true},"outputs":[],"execution_count":113},{"cell_type":"code","source":"# GDRIVE_ID_LSTM_MODEL = \"1kLRQ3o7m57qK1OJOTA-K9OBYL29zuXjo\"\n# !gdown $GDRIVE_ID_LSTM_MODEL -O LSTM.joblib","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:42:49.120694Z","iopub.execute_input":"2025-11-29T04:42:49.120928Z","iopub.status.idle":"2025-11-29T04:42:49.137249Z","shell.execute_reply.started":"2025-11-29T04:42:49.120913Z","shell.execute_reply":"2025-11-29T04:42:49.136436Z"},"trusted":true},"outputs":[],"execution_count":114},{"cell_type":"code","source":"# import tarfile\n\n# file_path = \"/kaggle/input/tashkeela/Tashkeela-arabic-diacritized-text-utf8-0.3.tar.bz2\"\n# extract_path = \"/kaggle/working/tashkeela_extracted\"\n\n# # Extract tar.bz2 file\n# with tarfile.open(file_path, \"r:bz2\") as tar:\n#     tar.extractall(path=extract_path)\n\n# extract_path","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:42:49.138791Z","iopub.execute_input":"2025-11-29T04:42:49.138978Z","iopub.status.idle":"2025-11-29T04:42:49.152920Z","shell.execute_reply.started":"2025-11-29T04:42:49.138964Z","shell.execute_reply":"2025-11-29T04:42:49.152384Z"},"trusted":true},"outputs":[],"execution_count":115},{"cell_type":"markdown","source":"<h1> Arabic letters and diacritics","metadata":{}},{"cell_type":"code","source":"import pickle\nimport re\nimport pyarabic.araby as araby\nimport pyarabic.number as number\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nimport unicodedata\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nimport joblib\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.callbacks import EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:42:49.153561Z","iopub.execute_input":"2025-11-29T04:42:49.153719Z","iopub.status.idle":"2025-11-29T04:42:49.176087Z","shell.execute_reply.started":"2025-11-29T04:42:49.153705Z","shell.execute_reply":"2025-11-29T04:42:49.175585Z"},"trusted":true},"outputs":[],"execution_count":116},{"cell_type":"markdown","source":"<h2> Constants","metadata":{}},{"cell_type":"code","source":"window_size = 1500","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:42:49.177631Z","iopub.execute_input":"2025-11-29T04:42:49.177833Z","iopub.status.idle":"2025-11-29T04:42:49.192013Z","shell.execute_reply.started":"2025-11-29T04:42:49.177819Z","shell.execute_reply":"2025-11-29T04:42:49.191337Z"},"trusted":true},"outputs":[],"execution_count":117},{"cell_type":"markdown","source":"## GPU Configuration for Kaggle\nCheck GPU availability and configure TensorFlow to use GPU","metadata":{}},{"cell_type":"code","source":"# Check if GPU is available\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n\n# Get GPU details\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Set memory growth to avoid OOM errors\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        \n        print(f\"GPU detected: {gpus}\")\n        print(f\"GPU Name: {tf.test.gpu_device_name()}\")\n    except RuntimeError as e:\n        print(e)\nelse:\n    print(\"No GPU found. Training will use CPU.\")","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:42:49.192739Z","iopub.execute_input":"2025-11-29T04:42:49.193033Z","iopub.status.idle":"2025-11-29T04:42:49.213369Z","shell.execute_reply.started":"2025-11-29T04:42:49.193018Z","shell.execute_reply":"2025-11-29T04:42:49.212677Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Num GPUs Available:  2\nGPU detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\nGPU Name: /device:GPU:0\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1764391369.209563      47 gpu_device.cc:2022] Created device /device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1764391369.209792      47 gpu_device.cc:2022] Created device /device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}],"execution_count":118},{"cell_type":"markdown","source":"## Load Dictionaries and Create Mappings","metadata":{}},{"cell_type":"code","source":"arabic_letters = []\ndiacritics = []\ndiacritics_to_id = {}\nwith open('./utils/arabic_letters.pickle', 'rb') as f:\n    arabic_letters = pickle.load(f)\nwith open('./utils/diacritics.pickle', 'rb') as f:\n    diacritics = pickle.load(f)\nwith open('./utils/diacritic2id.pickle', 'rb') as f:\n    diacritics_to_id = pickle.load(f)\n\narabic_letters_sorted = sorted(arabic_letters)\nchar_to_id = {char: idx + 1 for idx, char in enumerate(arabic_letters_sorted)}\nchar_to_id['<PAD>'] = 0\nchar_to_id['UNK'] = len(char_to_id)","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:42:49.213993Z","iopub.execute_input":"2025-11-29T04:42:49.214216Z","iopub.status.idle":"2025-11-29T04:42:49.230081Z","shell.execute_reply.started":"2025-11-29T04:42:49.214197Z","shell.execute_reply":"2025-11-29T04:42:49.229333Z"},"trusted":true},"outputs":[],"execution_count":119},{"cell_type":"markdown","source":"<h2> Read train and val data","metadata":{}},{"cell_type":"code","source":"train_data = []\nval_data = []\ntest_data = []\nwith open('./data/train.txt', 'r', encoding='utf-8') as f:\n    lines = f.readlines()\n    for line in lines:\n        train_data.append(line.strip())\nwith open('./data/val.txt', 'r', encoding='utf-8') as f:\n    lines = f.readlines()\n    for line in lines:\n        val_data.append(line.strip())\nwith open('/kaggle/working/tashkeela_extracted/Tashkeela-arabic-diacritized-text-utf8-0.3/texts.txt/غذاء الألباب في شرح منظومة الآداب.txt', 'r', encoding='utf-8') as f:\n    lines = f.readlines()\n    for line in lines:\n        test_data.append(line.strip())\n        \nprint(len(train_data))\nprint(len(val_data))\nprint(len(test_data))","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:51:24.718984Z","iopub.execute_input":"2025-11-29T04:51:24.719557Z","iopub.status.idle":"2025-11-29T04:51:24.985014Z","shell.execute_reply.started":"2025-11-29T04:51:24.719532Z","shell.execute_reply":"2025-11-29T04:51:24.984395Z"},"trusted":true},"outputs":[{"name":"stdout","text":"50000\n2500\n12213\n","output_type":"stream"}],"execution_count":138},{"cell_type":"markdown","source":"<h2> Clean data","metadata":{}},{"cell_type":"code","source":"def clean_arabic_text(text):\n    \"\"\"\n    Clean text to keep only Arabic letters, diacritics, and spaces\n    \"\"\"\n    # Create a set of allowed characters (Arabic letters + diacritics + space)\n    allowed_chars = arabic_letters.union(diacritics, {' ', '\\t', '\\n'})\n    \n    # Filter the text to keep only allowed characters\n    cleaned_text = ''.join(char for char in text if char in allowed_chars)\n    \n    # Normalize whitespace\n    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n    \n    return cleaned_text\n\n\ndef split_sentences(sentences, window_size=window_size):\n    all_segments = []\n    \n    for sentence in sentences:\n        words = araby.tokenize(sentence)\n        current_segment = []\n        current_len = 0\n        \n        for word in words:\n            word_len = len(word)\n            add_space = 1 if current_segment else 0\n            \n            if current_len + word_len + add_space <= window_size:\n                current_segment.append(word)\n                current_len += word_len + add_space\n            else:\n                # save the segment\n                if current_segment:\n                    all_segments.append(\" \".join(current_segment))\n                \n                # start new segment\n                current_segment = [word]\n                current_len = word_len\n        \n        # append the final segment of the sentence\n        if current_segment:\n            all_segments.append(\" \".join(current_segment))\n\n    return all_segments\n\n\n\ndef sentence_tokeniz(sentences):\n    tokenized_sentences = []\n    for sentence in sentences:\n        subsentences = araby.sentence_tokenize(sentence)\n        tokenized_sentences.extend(subsentences)\n    return tokenized_sentences\n        \n","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:51:30.313177Z","iopub.execute_input":"2025-11-29T04:51:30.313442Z","iopub.status.idle":"2025-11-29T04:51:30.320496Z","shell.execute_reply.started":"2025-11-29T04:51:30.313426Z","shell.execute_reply":"2025-11-29T04:51:30.319597Z"},"trusted":true},"outputs":[],"execution_count":139},{"cell_type":"code","source":"train_data = sentence_tokeniz(train_data)\nval_data = sentence_tokeniz(val_data)\ntest_data = sentence_tokeniz(test_data)\n\nfor i in range(len(train_data)):\n    train_data[i] = clean_arabic_text(train_data[i])\nfor i in range(len(val_data)):\n    val_data[i] = clean_arabic_text(val_data[i])\nfor i in range(len(test_data)):\n    test_data[i] = clean_arabic_text(test_data[i])\n\ntrain_data = split_sentences(train_data, window_size)\nval_data = split_sentences(val_data, window_size)\ntest_data = split_sentences(test_data, window_size)","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:51:35.327011Z","iopub.execute_input":"2025-11-29T04:51:35.327753Z","iopub.status.idle":"2025-11-29T04:51:44.509378Z","shell.execute_reply.started":"2025-11-29T04:51:35.327728Z","shell.execute_reply":"2025-11-29T04:51:44.508798Z"},"trusted":true},"outputs":[],"execution_count":140},{"cell_type":"code","source":"def is_diacritic(ch):\n    # Unicode combining marks (Arabic diacritics are combining marks)\n    return unicodedata.combining(ch) != 0\n\ndef extract_base_and_diacritics(text):\n    # normalize to NFC so base+combining marks are consistent\n    text = unicodedata.normalize('NFC', text)\n    bases = []\n    diacs = []\n    current_base = None\n    current_diac = ''\n    for ch in text:\n        if is_diacritic(ch):\n            # accumulate diacritics for current base\n            current_diac += ch\n        else:\n            # new base character\n            if current_base is not None:\n                bases.append(current_base)\n                diacs.append(current_diac)\n            current_base = ch\n            current_diac = ''\n    # append last\n    if current_base is not None:\n        bases.append(current_base)\n        diacs.append(current_diac)\n    return bases, diacs","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:51:47.599052Z","iopub.execute_input":"2025-11-29T04:51:47.599325Z","iopub.status.idle":"2025-11-29T04:51:47.606221Z","shell.execute_reply.started":"2025-11-29T04:51:47.599293Z","shell.execute_reply":"2025-11-29T04:51:47.605150Z"},"trusted":true},"outputs":[],"execution_count":143},{"cell_type":"markdown","source":"<h2> Prepare data for the model","metadata":{}},{"cell_type":"code","source":"# Pad sequences to same length\nPAD_DIACRITIC_ID = diacritics_to_id.get('', 0)  # Use empty string diacritic for padding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T04:51:51.649579Z","iopub.execute_input":"2025-11-29T04:51:51.650168Z","iopub.status.idle":"2025-11-29T04:51:51.653585Z","shell.execute_reply.started":"2025-11-29T04:51:51.650144Z","shell.execute_reply":"2025-11-29T04:51:51.652859Z"}},"outputs":[],"execution_count":144},{"cell_type":"code","source":"# Prepare training data - extract characters and their diacritics\nx_train_raw = []\ny_train_raw = []\n\n# Use a constant for unknown diacritic instead of hardcoded value\nUNKNOWN_DIACRITIC_ID = diacritics_to_id.get('', len(diacritics_to_id) - 1)\n\nfor text in train_data:\n    bases, diacs = extract_base_and_diacritics(text)\n    # convert letters to IDs\n    x_train_raw.append([char_to_id.get(c, char_to_id['UNK']) for c in bases])\n    y_train_raw.append([diacritics_to_id.get(d, UNKNOWN_DIACRITIC_ID) for d in diacs])\n\n","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:51:54.924497Z","iopub.execute_input":"2025-11-29T04:51:54.924970Z","iopub.status.idle":"2025-11-29T04:52:01.688033Z","shell.execute_reply.started":"2025-11-29T04:51:54.924942Z","shell.execute_reply":"2025-11-29T04:52:01.687444Z"},"trusted":true},"outputs":[],"execution_count":145},{"cell_type":"code","source":"# Pad validation sequences\nx_train = tf.keras.preprocessing.sequence.pad_sequences(x_train_raw, padding='post', value=0)\ny_train = tf.keras.preprocessing.sequence.pad_sequences(y_train_raw, padding='post', value=PAD_DIACRITIC_ID)\n\nprint(f\"x_train shape: {x_train.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:52:01.689087Z","iopub.execute_input":"2025-11-29T04:52:01.689617Z","iopub.status.idle":"2025-11-29T04:52:03.576815Z","shell.execute_reply.started":"2025-11-29T04:52:01.689595Z","shell.execute_reply":"2025-11-29T04:52:03.575979Z"},"trusted":true},"outputs":[{"name":"stdout","text":"x_train shape: (171895, 909)\ny_train shape: (171895, 909)\n","output_type":"stream"}],"execution_count":146},{"cell_type":"code","source":"# Prepare validation data\nx_val_raw = []\ny_val_raw = []\n\nfor text in val_data:\n    bases, diacs = extract_base_and_diacritics(text)\n    # convert letters to IDs\n    x_val_raw.append([char_to_id.get(c, char_to_id['UNK']) for c in bases])\n    y_val_raw.append([diacritics_to_id.get(d, UNKNOWN_DIACRITIC_ID) for d in diacs])","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:52:05.126221Z","iopub.execute_input":"2025-11-29T04:52:05.126524Z","iopub.status.idle":"2025-11-29T04:52:05.433517Z","shell.execute_reply.started":"2025-11-29T04:52:05.126501Z","shell.execute_reply":"2025-11-29T04:52:05.432725Z"},"trusted":true},"outputs":[],"execution_count":147},{"cell_type":"code","source":"# Pad validation sequences\nx_val = tf.keras.preprocessing.sequence.pad_sequences(x_val_raw, padding='post', value=0)\ny_val = tf.keras.preprocessing.sequence.pad_sequences(y_val_raw, padding='post', value=PAD_DIACRITIC_ID)\n\nprint(f\"x_val shape: {x_val.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:52:09.997985Z","iopub.execute_input":"2025-11-29T04:52:09.998586Z","iopub.status.idle":"2025-11-29T04:52:10.098673Z","shell.execute_reply.started":"2025-11-29T04:52:09.998565Z","shell.execute_reply":"2025-11-29T04:52:10.097999Z"},"trusted":true},"outputs":[{"name":"stdout","text":"x_val shape: (8314, 899)\ny_val shape: (8314, 899)\n","output_type":"stream"}],"execution_count":149},{"cell_type":"code","source":"# Prepare test data\nx_test_raw = []\ny_test_raw = []\n\nfor text in test_data:\n    bases, diacs = extract_base_and_diacritics(text)\n    # convert letters to IDs\n    x_test_raw.append([char_to_id.get(c, char_to_id['UNK']) for c in bases])\n    y_test_raw.append([diacritics_to_id.get(d, UNKNOWN_DIACRITIC_ID) for d in diacs])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T04:52:13.430577Z","iopub.execute_input":"2025-11-29T04:52:13.431310Z","iopub.status.idle":"2025-11-29T04:52:14.400842Z","shell.execute_reply.started":"2025-11-29T04:52:13.431282Z","shell.execute_reply":"2025-11-29T04:52:14.400255Z"}},"outputs":[],"execution_count":150},{"cell_type":"code","source":"# Pad test sequences\nx_test = tf.keras.preprocessing.sequence.pad_sequences(x_test_raw, padding='post', value=0)\ny_test = tf.keras.preprocessing.sequence.pad_sequences(y_test_raw, padding='post', value=PAD_DIACRITIC_ID)\n\nprint(f\"x_test shape: {x_test.shape}\")\nprint(f\"y_test shape: {y_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T04:52:17.238167Z","iopub.execute_input":"2025-11-29T04:52:17.238623Z","iopub.status.idle":"2025-11-29T04:52:17.615682Z","shell.execute_reply.started":"2025-11-29T04:52:17.238603Z","shell.execute_reply":"2025-11-29T04:52:17.614500Z"}},"outputs":[{"name":"stdout","text":"x_test shape: (35302, 909)\ny_test shape: (35302, 909)\n","output_type":"stream"}],"execution_count":151},{"cell_type":"code","source":"vocab_size = len(char_to_id)\nnum_diacritics = len(diacritics_to_id)\n\nmodel = models.Sequential([\n    # TRAINABLE EMBEDDINGS (THIS LAYER LEARNS)\n    layers.Embedding(input_dim=vocab_size,\n                     output_dim=128,     # embedding size (trainable)\n                     mask_zero=True),\n\n    # BiLSTM for sequence modeling\n    layers.Bidirectional(layers.LSTM(128, return_sequences=True)),\n\n    # Predict diacritic for each character - use Dense directly instead of TimeDistributed\n    layers.Dense(num_diacritics, activation='softmax')\n])\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=\"adam\",\n              metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:43:13.050145Z","iopub.execute_input":"2025-11-29T04:43:13.050377Z","iopub.status.idle":"2025-11-29T04:43:13.074237Z","shell.execute_reply.started":"2025-11-29T04:43:13.050349Z","shell.execute_reply":"2025-11-29T04:43:13.073710Z"},"trusted":true},"outputs":[],"execution_count":131},{"cell_type":"code","source":"# Train model on GPU\nearly = EarlyStopping(\n    monitor='val_loss',\n    patience=3,\n    restore_best_weights=True\n)\n\nwith tf.device('/GPU:0'):\n    history = model.fit(x_train, y_train, \n                        epochs=10, \n                        batch_size=64,\n                        verbose=1)","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:43:13.074866Z","iopub.execute_input":"2025-11-29T04:43:13.075054Z","iopub.status.idle":"2025-11-29T04:48:30.994720Z","shell.execute_reply.started":"2025-11-29T04:43:13.075019Z","shell.execute_reply":"2025-11-29T04:48:30.993834Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/3\n\u001b[1m2686/2686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 39ms/step - accuracy: 0.1215 - loss: 0.5949\nEpoch 2/3\n\u001b[1m2686/2686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 39ms/step - accuracy: 0.9953 - loss: 0.2135\nEpoch 3/3\n\u001b[1m2686/2686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 39ms/step - accuracy: 0.9963 - loss: 0.1735\n","output_type":"stream"}],"execution_count":132},{"cell_type":"code","source":"joblib.dump(model, \"/kaggle/working/model1.joblib\")","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:48:30.996324Z","iopub.execute_input":"2025-11-29T04:48:30.996582Z","iopub.status.idle":"2025-11-29T04:48:31.061264Z","shell.execute_reply.started":"2025-11-29T04:48:30.996565Z","shell.execute_reply":"2025-11-29T04:48:31.060609Z"},"trusted":true},"outputs":[{"execution_count":133,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/model1.joblib']"},"metadata":{}}],"execution_count":133},{"cell_type":"code","source":"# model = joblib.load(\"model1.joblib\")","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:53:43.048609Z","iopub.execute_input":"2025-11-29T04:53:43.048879Z","iopub.status.idle":"2025-11-29T04:53:44.067618Z","shell.execute_reply.started":"2025-11-29T04:53:43.048860Z","shell.execute_reply":"2025-11-29T04:53:44.066838Z"},"trusted":true},"outputs":[],"execution_count":155},{"cell_type":"code","source":"def calculate_der_by_position(x_val, y_true, y_pred, char_to_id):\n    \"\"\"\n    Calculate DER separately for last characters and non-last characters in words\n    \n    Args:\n        x_val: Character sequences (samples × sequence_length)\n        y_true: Ground truth diacritic labels (samples × sequence_length)\n        y_pred: Predicted diacritic labels (samples × sequence_length)\n        char_to_id: Dictionary mapping characters to IDs\n    \n    Returns:\n        Tuple of (DER_non_last, DER_last, overall_DER)\n    \"\"\"\n    # Get space character ID\n    space_id = char_to_id.get(' ', char_to_id.get('UNK'))\n    pad_id = char_to_id.get('<PAD>', 0)\n    \n    non_last_errors = 0\n    non_last_total = 0\n    last_errors = 0\n    last_total = 0\n    \n    # Process each sequence\n    for char_seq, y_true_seq, y_pred_seq in zip(x_val, y_true, y_pred):\n        # Find valid (non-padding) characters\n        valid_mask = char_seq != pad_id\n        valid_indices = np.where(valid_mask)[0]\n        \n        if len(valid_indices) == 0:\n            continue\n        \n        # Identify word boundaries (spaces and end of sequence)\n        i = 0\n        while i < len(valid_indices):\n            idx = valid_indices[i]\n            \n            # Skip spaces\n            if char_seq[idx] == space_id:\n                i += 1\n                continue\n            \n            # Find the end of current word\n            word_start = i\n            while i < len(valid_indices) and char_seq[valid_indices[i]] != space_id:\n                i += 1\n            word_end = i - 1\n            \n            # Mark positions in the word\n            for j in range(word_start, word_end + 1):\n                pos_idx = valid_indices[j]\n                \n                # Skip if this position has padding in y_true\n                if y_true_seq[pos_idx] == PAD_DIACRITIC_ID:\n                    continue\n                \n                is_correct = (y_true_seq[pos_idx] == y_pred_seq[pos_idx])\n                \n                # Last character in word\n                if j == word_end:\n                    last_total += 1\n                    if not is_correct:\n                        last_errors += 1\n                # Non-last character in word\n                else:\n                    non_last_total += 1\n                    if not is_correct:\n                        non_last_errors += 1\n    \n    # Calculate DER for each category\n    der_non_last = (non_last_errors / non_last_total * 100) if non_last_total > 0 else 0\n    der_last = (last_errors / last_total * 100) if last_total > 0 else 0\n    \n    total_errors = non_last_errors + last_errors\n    total_chars = non_last_total + last_total\n    der_overall = (total_errors / total_chars * 100) if total_chars > 0 else 0\n    \n    return der_non_last, der_last, der_overall","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:53:46.425327Z","iopub.execute_input":"2025-11-29T04:53:46.425844Z","iopub.status.idle":"2025-11-29T04:53:46.434183Z","shell.execute_reply.started":"2025-11-29T04:53:46.425822Z","shell.execute_reply":"2025-11-29T04:53:46.433447Z"},"trusted":true,"scrolled":true,"_kg_hide-output":false},"outputs":[],"execution_count":156},{"cell_type":"code","source":"y_pred = model.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:53:51.619633Z","iopub.execute_input":"2025-11-29T04:53:51.620145Z","iopub.status.idle":"2025-11-29T04:54:05.099738Z","shell.execute_reply.started":"2025-11-29T04:53:51.620124Z","shell.execute_reply":"2025-11-29T04:54:05.098987Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[1m1104/1104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step\n","output_type":"stream"}],"execution_count":157},{"cell_type":"code","source":"y_pred_classes = np.argmax(y_pred, axis=-1)\ny_true = y_test\n\n# Calculate accuracy\naccuracy = accuracy_score(y_true.flatten(), y_pred_classes.flatten())\nprint(f'Test Accuracy: {accuracy:.4f}')","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:54:05.101586Z","iopub.execute_input":"2025-11-29T04:54:05.101847Z","iopub.status.idle":"2025-11-29T04:54:06.383145Z","shell.execute_reply.started":"2025-11-29T04:54:05.101830Z","shell.execute_reply":"2025-11-29T04:54:06.382482Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Test Accuracy: 0.9964\n","output_type":"stream"}],"execution_count":158},{"cell_type":"code","source":"# Calculate DER by character position in words\nder_non_last, der_last, der_overall = calculate_der_by_position(x_test, y_true, y_pred_classes, char_to_id)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"DER Analysis by Character Position in Words\")\nprint(\"=\"*60)\nprint(f\"DER for non-last characters: {der_non_last:.2f}%\")\nprint(f\"DER for last characters:     {der_last:.2f}%\")\nprint(f\"Overall DER:                 {der_overall:.2f}%\")\nprint(f\"\\nAccuracy for non-last characters: {100 - der_non_last:.2f}%\")\nprint(f\"Accuracy for last characters:     {100 - der_last:.2f}%\")\nprint(f\"Acutual Accuracy: {100 - der_overall:.2f}%\")\nprint(\"=\"*60)","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:54:06.383916Z","iopub.execute_input":"2025-11-29T04:54:06.384106Z","iopub.status.idle":"2025-11-29T04:54:12.860167Z","shell.execute_reply.started":"2025-11-29T04:54:06.384091Z","shell.execute_reply":"2025-11-29T04:54:12.859333Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n============================================================\nDER Analysis by Character Position in Words\n============================================================\nDER for non-last characters: 9.01%\nDER for last characters:     14.80%\nOverall DER:                 10.38%\n\nAccuracy for non-last characters: 90.99%\nAccuracy for last characters:     85.20%\nAcutual Accuracy: 89.62%\n============================================================\n","output_type":"stream"}],"execution_count":159},{"cell_type":"markdown","source":"<h2> Test data","metadata":{}},{"cell_type":"code","source":"def merge(x_test, y_pred_classes, char_to_id, diacritics_to_id):\n    \"\"\"\n    Merge character sequences with predicted diacritics to reconstruct text\n    \n    Args:\n        x_test: numpy array of character IDs (samples × sequence_length)\n        y_pred_classes: numpy array of predicted diacritic IDs (samples × sequence_length)\n        char_to_id: dictionary mapping characters to IDs\n        diacritics_to_id: dictionary mapping diacritics to IDs\n    \n    Returns:\n        List of reconstructed diacritized text strings\n    \"\"\"\n    # Create reverse mappings\n    id_to_char = {v: k for k, v in char_to_id.items()}\n    id_to_diacritic = {v: k for k, v in diacritics_to_id.items()}\n    \n    reconstructed_texts = []\n    \n    # Process each sample\n    for char_seq, diac_seq in zip(x_test, y_pred_classes):\n        text = \"\"\n        \n        for char_id, diac_id in zip(char_seq, diac_seq):\n            # Skip padding\n            if char_id == 0:  # PAD character\n                break\n            \n            # Get character\n            char = id_to_char.get(char_id, '')\n            \n            # Get diacritic\n            diacritic = id_to_diacritic.get(diac_id, '')\n            \n            # Combine character with diacritic\n            text += char + diacritic\n        \n        text = text.replace(\"UNK\", \" \")\n        reconstructed_texts.append(text)\n    \n    return reconstructed_texts","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:49:15.157545Z","iopub.status.idle":"2025-11-29T04:49:15.157871Z","shell.execute_reply.started":"2025-11-29T04:49:15.157756Z","shell.execute_reply":"2025-11-29T04:49:15.157767Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_sent = \"هذا نص تجريبي لاختبار نموذج تشكيل النص العربي.\"\n\nx_test = test_model(test_sent)\ny_test_pred = model.predict(np.array(x_test))\ny_test_pred_classes = np.argmax(y_test_pred, axis=-1)","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:49:15.158805Z","iopub.status.idle":"2025-11-29T04:49:15.159032Z","shell.execute_reply.started":"2025-11-29T04:49:15.158912Z","shell.execute_reply":"2025-11-29T04:49:15.158920Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_test_pred.shape","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:49:15.159840Z","iopub.status.idle":"2025-11-29T04:49:15.160120Z","shell.execute_reply.started":"2025-11-29T04:49:15.159962Z","shell.execute_reply":"2025-11-29T04:49:15.159974Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_sentences = merge(x_test, y_test_pred_classes, char_to_id, diacritics_to_id)[0]\nprint(output_sentences)","metadata":{"execution":{"iopub.status.busy":"2025-11-29T04:49:15.160915Z","iopub.status.idle":"2025-11-29T04:49:15.161139Z","shell.execute_reply.started":"2025-11-29T04:49:15.161035Z","shell.execute_reply":"2025-11-29T04:49:15.161043Z"},"trusted":true},"outputs":[],"execution_count":null}]}