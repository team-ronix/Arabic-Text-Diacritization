{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# GDRIVE_ID_DATA = \"1ONRQ36PFPnYNA4R6ZlmM7UQJ4LiAzEH0\"\n",
    "# !gdown $GDRIVE_ID_DATA -O Arabic-Text-Diacritization.zip\n",
    "# !unzip Arabic-Text-Diacritization.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import tarfile\n",
    "\n",
    "# file_path = \"/kaggle/input/tashkeela/Tashkeela-arabic-diacritized-text-utf8-0.3.tar.bz2\"\n",
    "# extract_path = \"/kaggle/working/tashkeela_extracted\"\n",
    "\n",
    "# # Extract tar.bz2 file\n",
    "# with tarfile.open(file_path, \"r:bz2\") as tar:\n",
    "#     tar.extractall(path=extract_path)\n",
    "\n",
    "# extract_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Arabic letters and diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:35:10.623149Z",
     "iopub.status.busy": "2025-11-30T19:35:10.622168Z",
     "iopub.status.idle": "2025-11-30T19:35:10.627763Z",
     "shell.execute_reply": "2025-11-30T19:35:10.627116Z",
     "shell.execute_reply.started": "2025-11-30T19:35:10.623116Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import pyarabic.araby as araby\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input, Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:35:13.073022Z",
     "iopub.status.busy": "2025-11-30T19:35:13.072245Z",
     "iopub.status.idle": "2025-11-30T19:35:13.076179Z",
     "shell.execute_reply": "2025-11-30T19:35:13.075558Z",
     "shell.execute_reply.started": "2025-11-30T19:35:13.072996Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "window_size = 1000\n",
    "\n",
    "ARABIC_LETTERS_PATH = './utils/arabic_letters.pickle'\n",
    "DIACRITICS_PATH = './utils/diacritics.pickle'\n",
    "DIACRITICS_TO_ID_PATH = './utils/diacritic2id.pickle'\n",
    "CHAR_TO_ID_PATH = './utils/char2id.pickle'\n",
    "WORD_TO_ID_PATH = './utils/word2id.pickle'\n",
    "\n",
    "TRAIN_PATH = './data/train.txt'\n",
    "VAL_PATH = './data/val.txt'\n",
    "\n",
    "MODEL_SAVE_DIR = \"./models\"\n",
    "MODEL_WEIGHTS_PATH = './models/best_weights.ckpt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Configuration for Kaggle\n",
    "Check GPU availability and configure TensorFlow to use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:35:15.461154Z",
     "iopub.status.busy": "2025-11-30T19:35:15.460879Z",
     "iopub.status.idle": "2025-11-30T19:35:15.466776Z",
     "shell.execute_reply": "2025-11-30T19:35:15.465953Z",
     "shell.execute_reply.started": "2025-11-30T19:35:15.461136Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "No GPU found. Training will use CPU.\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        print(f\"GPU detected: {gpus}\")\n",
    "        print(f\"GPU Name: {tf.test.gpu_device_name()}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found. Training will use CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dictionaries and Create Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:35:17.417719Z",
     "iopub.status.busy": "2025-11-30T19:35:17.417420Z",
     "iopub.status.idle": "2025-11-30T19:35:17.423424Z",
     "shell.execute_reply": "2025-11-30T19:35:17.422826Z",
     "shell.execute_reply.started": "2025-11-30T19:35:17.417699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "arabic_letters = []\n",
    "diacritics = []\n",
    "diacritics_to_id = {}\n",
    "\n",
    "with open(ARABIC_LETTERS_PATH, 'rb') as f:\n",
    "    arabic_letters = pickle.load(f)\n",
    "with open(DIACRITICS_PATH, 'rb') as f:\n",
    "    diacritics = pickle.load(f)\n",
    "with open(DIACRITICS_TO_ID_PATH, 'rb') as f:\n",
    "    diacritics_to_id = pickle.load(f)\n",
    "    \n",
    "arabic_letters_sorted = sorted(arabic_letters)\n",
    "char_to_id = {char: idx + 1 for idx, char in enumerate(arabic_letters_sorted)}\n",
    "char_to_id['<PAD>'] = 0\n",
    "char_to_id['UNK'] = len(char_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:35:19.177455Z",
     "iopub.status.busy": "2025-11-30T19:35:19.177147Z",
     "iopub.status.idle": "2025-11-30T19:35:19.187529Z",
     "shell.execute_reply": "2025-11-30T19:35:19.186729Z",
     "shell.execute_reply.started": "2025-11-30T19:35:19.177433Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_word_vocabulary(data):\n",
    "    \"\"\"\n",
    "    Build word vocabulary from training data\n",
    "    \n",
    "    Args:\n",
    "        data: List of text samples\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping words to IDs\n",
    "    \"\"\"\n",
    "    word_counts = {}\n",
    "    for text in data:\n",
    "        text_no_diac = araby.strip_diacritics(text)\n",
    "        words = araby.tokenize(text_no_diac)\n",
    "        for word in words:\n",
    "            if word.strip():  \n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    \n",
    "    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    word_to_id = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for idx, (word, _) in enumerate(sorted_words):\n",
    "        word_to_id[word] = idx + 2\n",
    "    \n",
    "    return word_to_id\n",
    "\n",
    "word_to_id = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Read train and val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:35:21.238759Z",
     "iopub.status.busy": "2025-11-30T19:35:21.238468Z",
     "iopub.status.idle": "2025-11-30T19:35:21.431888Z",
     "shell.execute_reply": "2025-11-30T19:35:21.431090Z",
     "shell.execute_reply.started": "2025-11-30T19:35:21.238739Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "val_data = []\n",
    "with open(TRAIN_PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        train_data.append(line.strip())\n",
    "with open(VAL_PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        val_data.append(line.strip())\n",
    "print(len(train_data))\n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:35:23.195708Z",
     "iopub.status.busy": "2025-11-30T19:35:23.195409Z",
     "iopub.status.idle": "2025-11-30T19:35:23.205778Z",
     "shell.execute_reply": "2025-11-30T19:35:23.204948Z",
     "shell.execute_reply.started": "2025-11-30T19:35:23.195685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_arabic_text(text):\n",
    "    \"\"\"\n",
    "    Clean text to keep only Arabic letters, diacritics, and spaces\n",
    "    \"\"\"\n",
    "    allowed_chars = arabic_letters.union(diacritics, {' ', '\\t', '\\n'})\n",
    "    \n",
    "    cleaned_text = ''.join(char for char in text if char in allowed_chars)\n",
    "    \n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def split_sentences(sentences, window_size=window_size):\n",
    "    all_segments = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = araby.tokenize(sentence)\n",
    "        current_segment = []\n",
    "        current_len = 0\n",
    "        \n",
    "        for word in words:\n",
    "            word_len = len(word)\n",
    "            add_space = 1 if current_segment else 0\n",
    "            \n",
    "            if current_len + word_len + add_space <= window_size:\n",
    "                current_segment.append(word)\n",
    "                current_len += word_len + add_space\n",
    "            else:\n",
    "                if current_segment:\n",
    "                    all_segments.append(\" \".join(current_segment))\n",
    "                \n",
    "                current_segment = [word]\n",
    "                current_len = word_len\n",
    "        \n",
    "        if current_segment:\n",
    "            all_segments.append(\" \".join(current_segment))\n",
    "\n",
    "    return all_segments\n",
    "\n",
    "\n",
    "def sentence_tokeniz(sentences):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        subsentences = araby.sentence_tokenize(sentence)\n",
    "        tokenized_sentences.extend(subsentences)\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:35:25.681750Z",
     "iopub.status.busy": "2025-11-30T19:35:25.681113Z",
     "iopub.status.idle": "2025-11-30T19:35:38.010554Z",
     "shell.execute_reply": "2025-11-30T19:35:38.009839Z",
     "shell.execute_reply.started": "2025-11-30T19:35:25.681726Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building word vocabulary...\n",
      "Word vocabulary size: 105864\n"
     ]
    }
   ],
   "source": [
    "train_data = sentence_tokeniz(train_data)\n",
    "val_data = sentence_tokeniz(val_data)\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i] = clean_arabic_text(train_data[i])\n",
    "for i in range(len(val_data)):\n",
    "    val_data[i] = clean_arabic_text(val_data[i])\n",
    "\n",
    "train_data = split_sentences(train_data, window_size)\n",
    "val_data = split_sentences(val_data, window_size)\n",
    "\n",
    "print(\"Building word vocabulary...\")\n",
    "word_to_id = build_word_vocabulary(train_data)\n",
    "print(f\"Word vocabulary size: {len(word_to_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:35:38.012241Z",
     "iopub.status.busy": "2025-11-30T19:35:38.011933Z",
     "iopub.status.idle": "2025-11-30T19:35:38.017541Z",
     "shell.execute_reply": "2025-11-30T19:35:38.016755Z",
     "shell.execute_reply.started": "2025-11-30T19:35:38.012223Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def is_diacritic(ch):\n",
    "    return unicodedata.combining(ch) != 0\n",
    "\n",
    "def extract_base_and_diacritics(text):\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    bases = []\n",
    "    diacs = []\n",
    "    current_base = None\n",
    "    current_diac = ''\n",
    "    for ch in text:\n",
    "        if is_diacritic(ch):\n",
    "            current_diac += ch\n",
    "        else:\n",
    "            if current_base is not None:\n",
    "                bases.append(current_base)\n",
    "                diacs.append(current_diac)\n",
    "            current_base = ch\n",
    "            current_diac = ''\n",
    "    if current_base is not None:\n",
    "        bases.append(current_base)\n",
    "        diacs.append(current_diac)\n",
    "    return bases, diacs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:35:38.018682Z",
     "iopub.status.busy": "2025-11-30T19:35:38.018441Z",
     "iopub.status.idle": "2025-11-30T19:35:38.033232Z",
     "shell.execute_reply": "2025-11-30T19:35:38.032406Z",
     "shell.execute_reply.started": "2025-11-30T19:35:38.018664Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_char_and_word_features(text, word_to_id):\n",
    "    \"\"\"\n",
    "    Extract both character-level and word-level features from text\n",
    "    \n",
    "    Args:\n",
    "        text: Input text with diacritics\n",
    "        word_to_id: Dictionary mapping words to IDs\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (char_ids, diacritic_ids, word_ids, word_positions)\n",
    "        - char_ids: List of character IDs\n",
    "        - diacritic_ids: List of diacritic IDs for each character\n",
    "        - word_ids: List of word IDs aligned with characters\n",
    "        - word_positions: List indicating position in word (0=not end, 1=end, 2=space)\n",
    "    \"\"\"\n",
    "    bases, diacs = extract_base_and_diacritics(text)\n",
    "    \n",
    "    UNKNOWN_DIACRITIC_ID = diacritics_to_id.get('', len(diacritics_to_id) - 1)\n",
    "\n",
    "    char_ids = [char_to_id.get(c, char_to_id['UNK']) for c in bases]\n",
    "    diacritic_ids = [diacritics_to_id.get(d, UNKNOWN_DIACRITIC_ID) for d in diacs]\n",
    "    \n",
    "    text_no_diac = araby.strip_diacritics(text)\n",
    "    words = araby.tokenize(text_no_diac)\n",
    "    \n",
    "    word_ids = []\n",
    "    word_positions = [] # 0=not end, 1=end, 2=space\n",
    "    \n",
    "    char_idx = 0\n",
    "    for word in words:\n",
    "        if not word.strip():\n",
    "            continue\n",
    "        \n",
    "        word_id = word_to_id.get(word, word_to_id['<UNK>'])\n",
    "        word_len = len(word)\n",
    "        \n",
    "        for i in range(word_len):\n",
    "            if char_idx < len(char_ids):\n",
    "                word_ids.append(word_id)\n",
    "                if i == word_len - 1:\n",
    "                    word_positions.append(1)\n",
    "                else:\n",
    "                    word_positions.append(0)\n",
    "                char_idx += 1\n",
    "        \n",
    "        if char_idx < len(char_ids) and bases[char_idx] == ' ':\n",
    "            word_ids.append(0)  # for padding\n",
    "            word_positions.append(2)\n",
    "            char_idx += 1\n",
    "    \n",
    "    while len(word_ids) < len(char_ids):\n",
    "        word_ids.append(0)\n",
    "        word_positions.append(2)\n",
    "    \n",
    "    return char_ids, diacritic_ids, word_ids, word_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(char_to_id, open(CHAR_TO_ID_PATH, 'wb'))\n",
    "pickle.dump(word_to_id, open(WORD_TO_ID_PATH, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prepare data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:35:38.034862Z",
     "iopub.status.busy": "2025-11-30T19:35:38.034663Z",
     "iopub.status.idle": "2025-11-30T19:35:52.969153Z",
     "shell.execute_reply": "2025-11-30T19:35:52.968352Z",
     "shell.execute_reply.started": "2025-11-30T19:35:38.034848Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting character and word-level features from training data...\n",
      "Training samples: 172467\n"
     ]
    }
   ],
   "source": [
    "x_train_char_raw = []\n",
    "y_train_raw = []\n",
    "x_train_word_raw = []\n",
    "x_train_word_position_raw = []\n",
    "\n",
    "UNKNOWN_DIACRITIC_ID = diacritics_to_id.get('', len(diacritics_to_id) - 1)\n",
    "\n",
    "print(\"Extracting character and word-level features from training data...\")\n",
    "for text in train_data:\n",
    "    char_ids, diacritic_ids, word_ids, word_positions = extract_char_and_word_features(text, word_to_id)\n",
    "    \n",
    "    x_train_char_raw.append(char_ids)\n",
    "    y_train_raw.append(diacritic_ids)\n",
    "    x_train_word_raw.append(word_ids)\n",
    "    x_train_word_position_raw.append(word_positions)\n",
    "\n",
    "print(f\"Training samples: {len(x_train_char_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:35:52.970291Z",
     "iopub.status.busy": "2025-11-30T19:35:52.969993Z",
     "iopub.status.idle": "2025-11-30T19:35:56.722068Z",
     "shell.execute_reply": "2025-11-30T19:35:56.721237Z",
     "shell.execute_reply.started": "2025-11-30T19:35:52.970274Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_char shape: (172467, 607)\n",
      "x_train_word shape: (172467, 607)\n",
      "x_train_position shape: (172467, 607)\n",
      "y_train shape: (172467, 607)\n"
     ]
    }
   ],
   "source": [
    "PAD_DIACRITIC_ID = diacritics_to_id.get('', 0)\n",
    "\n",
    "x_train_char = tf.keras.preprocessing.sequence.pad_sequences(x_train_char_raw, padding='post', value=0)\n",
    "x_train_word = tf.keras.preprocessing.sequence.pad_sequences(x_train_word_raw, padding='post', value=0)\n",
    "x_train_position = tf.keras.preprocessing.sequence.pad_sequences(x_train_word_position_raw, padding='post', value=2)\n",
    "y_train = tf.keras.preprocessing.sequence.pad_sequences(y_train_raw, padding='post', value=PAD_DIACRITIC_ID)\n",
    "\n",
    "print(f\"x_train_char shape: {x_train_char.shape}\")\n",
    "print(f\"x_train_word shape: {x_train_word.shape}\")\n",
    "print(f\"x_train_position shape: {x_train_position.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:35:56.723333Z",
     "iopub.status.busy": "2025-11-30T19:35:56.722958Z",
     "iopub.status.idle": "2025-11-30T19:35:57.365121Z",
     "shell.execute_reply": "2025-11-30T19:35:57.364515Z",
     "shell.execute_reply.started": "2025-11-30T19:35:56.723307Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting character and word-level features from validation data...\n",
      "Validation samples: 8332\n"
     ]
    }
   ],
   "source": [
    "x_val_char_raw = []\n",
    "y_val_raw = []\n",
    "x_val_word_raw = []\n",
    "x_val_word_position_raw = []\n",
    "\n",
    "print(\"Extracting character and word-level features from validation data...\")\n",
    "for text in val_data:\n",
    "    char_ids, diacritic_ids, word_ids, word_positions = extract_char_and_word_features(text, word_to_id)\n",
    "    \n",
    "    x_val_char_raw.append(char_ids)\n",
    "    y_val_raw.append(diacritic_ids)\n",
    "    x_val_word_raw.append(word_ids)\n",
    "    x_val_word_position_raw.append(word_positions)\n",
    "\n",
    "print(f\"Validation samples: {len(x_val_char_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:35:57.366738Z",
     "iopub.status.busy": "2025-11-30T19:35:57.366554Z",
     "iopub.status.idle": "2025-11-30T19:35:57.559085Z",
     "shell.execute_reply": "2025-11-30T19:35:57.558294Z",
     "shell.execute_reply.started": "2025-11-30T19:35:57.366724Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_val_char shape: (8332, 597)\n",
      "x_val_word shape: (8332, 597)\n",
      "x_val_position shape: (8332, 597)\n",
      "y_val shape: (8332, 597)\n"
     ]
    }
   ],
   "source": [
    "x_val_char = tf.keras.preprocessing.sequence.pad_sequences(x_val_char_raw, padding='post', value=0)\n",
    "x_val_word = tf.keras.preprocessing.sequence.pad_sequences(x_val_word_raw, padding='post', value=0)\n",
    "x_val_position = tf.keras.preprocessing.sequence.pad_sequences(x_val_word_position_raw, padding='post', value=2)\n",
    "y_val = tf.keras.preprocessing.sequence.pad_sequences(y_val_raw, padding='post', value=PAD_DIACRITIC_ID)\n",
    "\n",
    "print(f\"x_val_char shape: {x_val_char.shape}\")\n",
    "print(f\"x_val_word shape: {x_val_word.shape}\")\n",
    "print(f\"x_val_position shape: {x_val_position.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:35:57.560266Z",
     "iopub.status.busy": "2025-11-30T19:35:57.559992Z",
     "iopub.status.idle": "2025-11-30T19:35:57.567560Z",
     "shell.execute_reply": "2025-11-30T19:35:57.566730Z",
     "shell.execute_reply.started": "2025-11-30T19:35:57.560249Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, pad_id=0, space_id=None, name='DER', **kwargs):\n",
    "        super(DERMetric, self).__init__(name=name, **kwargs)\n",
    "        self.pad_id = pad_id\n",
    "        self.space_id = space_id\n",
    "        self.total = self.add_weight(name='total', initializer='zeros')\n",
    "        self.errors = self.add_weight(name='errors', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        y_true: shape (batch_size, seq_len)\n",
    "        y_pred: shape (batch_size, seq_len, num_classes)\n",
    "        \"\"\"\n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1, output_type=tf.int32)\n",
    "\n",
    "        mask = tf.not_equal(y_true, self.pad_id)\n",
    "        if self.space_id is not None:\n",
    "            mask = tf.logical_and(mask, tf.not_equal(y_true, self.space_id))\n",
    "\n",
    "        correct = tf.equal(y_true, y_pred_labels)\n",
    "        correct = tf.logical_and(correct, mask)\n",
    "\n",
    "        batch_errors = tf.reduce_sum(tf.cast(~correct, tf.float32))\n",
    "        batch_total = tf.reduce_sum(tf.cast(mask, tf.float32))\n",
    "\n",
    "        self.errors.assign_add(batch_errors)\n",
    "        self.total.assign_add(batch_total)\n",
    "\n",
    "    def result(self):\n",
    "        return (self.errors / self.total) * 100  # DER in %\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.errors.assign(0)\n",
    "        self.total.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_der_by_position(x_val_char, y_true, y_pred, char_to_id):\n",
    "    \"\"\"\n",
    "    Calculate DER separately for last characters and non-last characters in words\n",
    "    \n",
    "    Args:\n",
    "        x_val_char: Character sequences (samples x sequence_length)\n",
    "        y_true: Ground truth diacritic labels (samples x sequence_length)\n",
    "        y_pred: Predicted diacritic labels (samples x sequence_length)\n",
    "        char_to_id: Dictionary mapping characters to IDs\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (DER_non_last, DER_last, overall_DER)\n",
    "    \"\"\"\n",
    "    space_id = char_to_id.get(' ', char_to_id.get('UNK'))\n",
    "    pad_id = char_to_id.get('<PAD>', 0)\n",
    "    \n",
    "    non_last_errors = 0\n",
    "    non_last_total = 0\n",
    "    last_errors = 0\n",
    "    last_total = 0\n",
    "    \n",
    "    for char_seq, y_true_seq, y_pred_seq in zip(x_val_char, y_true, y_pred):\n",
    "        valid_mask = char_seq != pad_id\n",
    "        valid_indices = np.where(valid_mask)[0]\n",
    "        \n",
    "        if len(valid_indices) == 0:\n",
    "            continue\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(valid_indices):\n",
    "            idx = valid_indices[i]\n",
    "            \n",
    "            if char_seq[idx] == space_id:\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            word_start = i\n",
    "            while i < len(valid_indices) and char_seq[valid_indices[i]] != space_id:\n",
    "                i += 1\n",
    "            word_end = i - 1\n",
    "            \n",
    "            for j in range(word_start, word_end + 1):\n",
    "                pos_idx = valid_indices[j]\n",
    "                \n",
    "                if y_true_seq[pos_idx] == PAD_DIACRITIC_ID:\n",
    "                    continue\n",
    "                \n",
    "                is_correct = (y_true_seq[pos_idx] == y_pred_seq[pos_idx])\n",
    "                \n",
    "                if j == word_end:\n",
    "                    last_total += 1\n",
    "                    if not is_correct:\n",
    "                        last_errors += 1\n",
    "                else:\n",
    "                    non_last_total += 1\n",
    "                    if not is_correct:\n",
    "                        non_last_errors += 1\n",
    "    \n",
    "    der_non_last = (non_last_errors / non_last_total * 100) if non_last_total > 0 else 0\n",
    "    der_last = (last_errors / last_total * 100) if last_total > 0 else 0\n",
    "    \n",
    "    total_errors = non_last_errors + last_errors\n",
    "    total_chars = non_last_total + last_total\n",
    "    der_overall = (total_errors / total_chars * 100) if total_chars > 0 else 0\n",
    "    \n",
    "    return der_non_last, der_last, der_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:35:57.568559Z",
     "iopub.status.busy": "2025-11-30T19:35:57.568314Z",
     "iopub.status.idle": "2025-11-30T19:35:57.701693Z",
     "shell.execute_reply": "2025-11-30T19:35:57.701038Z",
     "shell.execute_reply.started": "2025-11-30T19:35:57.568544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, word_vocab_size, num_diacritics, pad_id):\n",
    "    char_input = Input(shape=(None,), name='char_input')\n",
    "    char_embedding = layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=128,\n",
    "        mask_zero=True,\n",
    "        name='char_embedding'\n",
    "    )(char_input)\n",
    "\n",
    "    word_input = Input(shape=(None,), name='word_input')\n",
    "    word_embedding = layers.Embedding(\n",
    "        input_dim=word_vocab_size,\n",
    "        output_dim=128,\n",
    "        mask_zero=True,\n",
    "        name='word_embedding'\n",
    "    )(word_input)\n",
    "\n",
    "    position_input = Input(shape=(None,), name='position_input')\n",
    "    position_embedding = layers.Embedding(\n",
    "        input_dim=3,\n",
    "        output_dim=16,\n",
    "        mask_zero=False,\n",
    "        name='position_embedding'\n",
    "    )(position_input)\n",
    "\n",
    "    combined = layers.Concatenate(name='feature_concat')([\n",
    "        char_embedding,\n",
    "        word_embedding,\n",
    "        position_embedding\n",
    "    ])\n",
    "\n",
    "    combined._keras_mask = char_embedding._keras_mask\n",
    "\n",
    "    lstm_out = layers.Bidirectional(\n",
    "        layers.LSTM(\n",
    "            256,\n",
    "            return_sequences=True,\n",
    "            activation='tanh',\n",
    "            recurrent_activation='sigmoid'\n",
    "        ),\n",
    "        name='bilstm'\n",
    "    )(combined)\n",
    "\n",
    "    output = layers.Dense(\n",
    "        num_diacritics,\n",
    "        activation='softmax',\n",
    "        name='diacritic_output'\n",
    "    )(lstm_out)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[char_input, word_input, position_input],\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "    der_metric = DERMetric(pad_id=0, space_id=pad_id)\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[der_metric]\n",
    "    )\n",
    "\n",
    "    print(\"\\nModel Architecture:\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ char_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_input      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ char_embedding      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,864</span> │ char_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_embedding      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">13,550,592</span> │ word_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_embedding  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span> │ position_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ feature_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ char_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ word_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ position_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ char_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bilstm              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,083,392</span> │ feature_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ diacritic_output    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,695</span> │ bilstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ char_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_input      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ char_embedding      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │      \u001b[38;5;34m4,864\u001b[0m │ char_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_embedding      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │ \u001b[38;5;34m13,550,592\u001b[0m │ word_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_embedding  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)  │         \u001b[38;5;34m48\u001b[0m │ position_input[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ feature_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m272\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ char_embedding[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ word_embedding[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ position_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ char_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bilstm              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │  \u001b[38;5;34m1,083,392\u001b[0m │ feature_concat[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ diacritic_output    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)  │      \u001b[38;5;34m7,695\u001b[0m │ bilstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,646,591</span> (55.87 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,646,591\u001b[0m (55.87 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,646,591</span> (55.87 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,646,591\u001b[0m (55.87 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(char_to_id),\n",
    "    word_vocab_size=len(word_to_id),\n",
    "    num_diacritics=len(diacritics_to_id),\n",
    "    pad_id=char_to_id.get('<PAD>', 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Callbacks for Training\n",
    "Configure early stopping and model checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:36:13.364257Z",
     "iopub.status.busy": "2025-11-30T19:36:13.363647Z",
     "iopub.status.idle": "2025-11-30T19:36:13.370959Z",
     "shell.execute_reply": "2025-11-30T19:36:13.370235Z",
     "shell.execute_reply.started": "2025-11-30T19:36:13.364228Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# if not os.path.exists(MODEL_SAVE_DIR):\n",
    "#     os.makedirs(MODEL_SAVE_DIR)\n",
    "\n",
    "# early_stopping = EarlyStopping(\n",
    "#     monitor='val_DER',           \n",
    "#     patience=3,                 \n",
    "#     mode='min',                 \n",
    "#     verbose=1,\n",
    "#     restore_best_weights=True    \n",
    "# )\n",
    "\n",
    "# checkpoint_path = MODEL_WEIGHTS_PATH\n",
    "# model_checkpoint = ModelCheckpoint(\n",
    "#     filepath=checkpoint_path,\n",
    "#     monitor='val_DER',           \n",
    "#     mode='min',                  \n",
    "#     save_best_only=True,    \n",
    "#     save_weights_only=True,       \n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# reduce_lr = ReduceLROnPlateau(\n",
    "#     monitor='val_DER',\n",
    "#     factor=0.5,                  \n",
    "#     patience=2,                  \n",
    "#     mode='min',\n",
    "#     verbose=1,\n",
    "#     min_lr=1e-7                \n",
    "# )\n",
    "\n",
    "# callbacks = [early_stopping, model_checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model with Validation and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:36:13.371918Z",
     "iopub.status.busy": "2025-11-30T19:36:13.371653Z",
     "iopub.status.idle": "2025-11-30T20:08:14.936994Z",
     "shell.execute_reply": "2025-11-30T20:08:14.936099Z",
     "shell.execute_reply.started": "2025-11-30T19:36:13.371896Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# with tf.device('/GPU:0'):\n",
    "#     history = model.fit(\n",
    "#         {'char_input': x_train_char, 'word_input': x_train_word, 'position_input': x_train_position},\n",
    "#         y_train,\n",
    "#         validation_data=(\n",
    "#             {'char_input': x_val_char, 'word_input': x_val_word, 'position_input': x_val_position},\n",
    "#             y_val\n",
    "#         ),\n",
    "#         epochs=50,                    \n",
    "#         batch_size=64,\n",
    "#         callbacks=callbacks,\n",
    "#         verbose=1\n",
    "#     )\n",
    "    \n",
    "# model.save_weights(\"weights.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Best Model \n",
    "Load the best model saved during training locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T20:42:33.998330Z",
     "iopub.status.busy": "2025-11-30T20:42:33.997588Z",
     "iopub.status.idle": "2025-11-30T20:42:34.720932Z",
     "shell.execute_reply": "2025-11-30T20:42:34.720271Z",
     "shell.execute_reply.started": "2025-11-30T20:42:33.998308Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model = build_model(\n",
    "#     vocab_size=len(char_to_id),\n",
    "#     word_vocab_size=len(word_to_id),\n",
    "#     num_diacritics=len(diacritics_to_id),\n",
    "#     pad_id=char_to_id.get('<PAD>', 0)\n",
    "# )\n",
    "# model.load_weights(MODEL_WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = './models/BiRNN_model.keras'\n",
    "model = tf.keras.models.load_model(MODEL_PATH, custom_objects={\"DERMetric\": DERMetric})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Test Val</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T20:42:42.479684Z",
     "iopub.status.busy": "2025-11-30T20:42:42.479370Z",
     "iopub.status.idle": "2025-11-30T20:42:47.868751Z",
     "shell.execute_reply": "2025-11-30T20:42:47.867918Z",
     "shell.execute_reply.started": "2025-11-30T20:42:42.479664Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 389ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict({'char_input': x_val_char, 'word_input': x_val_word, 'position_input': x_val_position})\n",
    "y_pred_classes = np.argmax(y_pred, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T20:42:51.872154Z",
     "iopub.status.busy": "2025-11-30T20:42:51.871908Z",
     "iopub.status.idle": "2025-11-30T20:42:52.080301Z",
     "shell.execute_reply": "2025-11-30T20:42:52.079669Z",
     "shell.execute_reply.started": "2025-11-30T20:42:51.872138Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9972\n"
     ]
    }
   ],
   "source": [
    "y_true = y_val\n",
    "\n",
    "accuracy = accuracy_score(y_true.flatten(), y_pred_classes.flatten())\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T20:42:54.151660Z",
     "iopub.status.busy": "2025-11-30T20:42:54.151039Z",
     "iopub.status.idle": "2025-11-30T20:42:56.141353Z",
     "shell.execute_reply": "2025-11-30T20:42:56.140626Z",
     "shell.execute_reply.started": "2025-11-30T20:42:54.151634Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DER Analysis by Character Position in Words\n",
      "============================================================\n",
      "DER for non-last characters: 3.17%\n",
      "DER for last characters:     6.43%\n",
      "Overall DER:                 3.95%\n",
      "\n",
      "Accuracy for non-last characters: 96.83%\n",
      "Accuracy for last characters:     93.57%\n",
      "Acutual Accuracy: 96.05%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "der_non_last, der_last, der_overall = calculate_der_by_position(x_val_char, y_true, y_pred_classes, char_to_id)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DER Analysis by Character Position in Words\")\n",
    "print(\"=\"*60)\n",
    "print(f\"DER for non-last characters: {der_non_last:.2f}%\")\n",
    "print(f\"DER for last characters:     {der_last:.2f}%\")\n",
    "print(f\"Overall DER:                 {der_overall:.2f}%\")\n",
    "print(f\"\\nAccuracy for non-last characters: {100 - der_non_last:.2f}%\")\n",
    "print(f\"Accuracy for last characters:     {100 - der_last:.2f}%\")\n",
    "print(f\"Total Accuracy: {100 - der_overall:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "IP2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
