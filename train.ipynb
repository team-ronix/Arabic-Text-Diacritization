{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:07.011339Z",
     "iopub.status.busy": "2025-12-09T12:08:07.010887Z",
     "iopub.status.idle": "2025-12-09T12:08:07.015228Z",
     "shell.execute_reply": "2025-12-09T12:08:07.014464Z",
     "shell.execute_reply.started": "2025-12-09T12:08:07.011313Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# GDRIVE_ID_DATA = \"1ONRQ36PFPnYNA4R6ZlmM7UQJ4LiAzEH0\"\n",
    "# !gdown $GDRIVE_ID_DATA -O Arabic-Text-Diacritization.zip\n",
    "# !unzip Arabic-Text-Diacritization.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:07.042049Z",
     "iopub.status.busy": "2025-12-09T12:08:07.041536Z",
     "iopub.status.idle": "2025-12-09T12:08:07.045279Z",
     "shell.execute_reply": "2025-12-09T12:08:07.044477Z",
     "shell.execute_reply.started": "2025-12-09T12:08:07.042020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import tarfile\n",
    "\n",
    "# file_path = \"/kaggle/input/tashkeela/Tashkeela-arabic-diacritized-text-utf8-0.3.tar.bz2\"\n",
    "# extract_path = \"/kaggle/working/tashkeela_extracted\"\n",
    "\n",
    "# # Extract tar.bz2 file\n",
    "# with tarfile.open(file_path, \"r:bz2\") as tar:\n",
    "#     tar.extractall(path=extract_path)\n",
    "\n",
    "# extract_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Arabic letters and diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:07.046444Z",
     "iopub.status.busy": "2025-12-09T12:08:07.046199Z",
     "iopub.status.idle": "2025-12-09T12:08:07.067602Z",
     "shell.execute_reply": "2025-12-09T12:08:07.066759Z",
     "shell.execute_reply.started": "2025-12-09T12:08:07.046424Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import pyarabic.araby as araby\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input, Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:07.069406Z",
     "iopub.status.busy": "2025-12-09T12:08:07.069120Z",
     "iopub.status.idle": "2025-12-09T12:08:07.089436Z",
     "shell.execute_reply": "2025-12-09T12:08:07.088625Z",
     "shell.execute_reply.started": "2025-12-09T12:08:07.069380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "window_size = 1000\n",
    "\n",
    "ARABIC_LETTERS_PATH = './utils/arabic_letters.pickle'\n",
    "DIACRITICS_PATH = './utils/diacritics.pickle'\n",
    "DIACRITICS_TO_ID_PATH = './utils/diacritic2id.pickle'\n",
    "CHAR_TO_ID_PATH = './utils/char2id.pickle'\n",
    "WORD_TO_ID_PATH = './utils/word2id.pickle'\n",
    "\n",
    "TRAIN_PATH = './data/train.txt'\n",
    "VAL_PATH = './data/val.txt'\n",
    "\n",
    "MODEL_SAVE_DIR = \"./models\"\n",
    "MODEL_WEIGHTS_PATH = './models/best_weights.keras'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Configuration for Kaggle\n",
    "Check GPU availability and configure TensorFlow to use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:07.090725Z",
     "iopub.status.busy": "2025-12-09T12:08:07.090399Z",
     "iopub.status.idle": "2025-12-09T12:08:07.110563Z",
     "shell.execute_reply": "2025-12-09T12:08:07.109835Z",
     "shell.execute_reply.started": "2025-12-09T12:08:07.090704Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n",
      "GPU detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "GPU Name: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765282087.107092      47 gpu_device.cc:2022] Created device /device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1765282087.107326      47 gpu_device.cc:2022] Created device /device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        print(f\"GPU detected: {gpus}\")\n",
    "        print(f\"GPU Name: {tf.test.gpu_device_name()}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found. Training will use CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dictionaries and Create Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:07.112019Z",
     "iopub.status.busy": "2025-12-09T12:08:07.111818Z",
     "iopub.status.idle": "2025-12-09T12:08:07.125071Z",
     "shell.execute_reply": "2025-12-09T12:08:07.124520Z",
     "shell.execute_reply.started": "2025-12-09T12:08:07.112003Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "arabic_letters = []\n",
    "diacritics = []\n",
    "diacritics_to_id = {}\n",
    "\n",
    "with open(ARABIC_LETTERS_PATH, 'rb') as f:\n",
    "    arabic_letters = pickle.load(f)\n",
    "with open(DIACRITICS_PATH, 'rb') as f:\n",
    "    diacritics = pickle.load(f)\n",
    "with open(DIACRITICS_TO_ID_PATH, 'rb') as f:\n",
    "    diacritics_to_id = pickle.load(f)\n",
    "    \n",
    "arabic_letters_sorted = sorted(arabic_letters)\n",
    "char_to_id = {char: idx + 1 for idx, char in enumerate(arabic_letters_sorted)}\n",
    "char_to_id['<PAD>'] = 0\n",
    "char_to_id['UNK'] = len(char_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:07.126039Z",
     "iopub.status.busy": "2025-12-09T12:08:07.125807Z",
     "iopub.status.idle": "2025-12-09T12:08:07.147382Z",
     "shell.execute_reply": "2025-12-09T12:08:07.146475Z",
     "shell.execute_reply.started": "2025-12-09T12:08:07.126023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_word_vocabulary(data):\n",
    "    \"\"\"\n",
    "    Build word vocabulary from training data\n",
    "    \n",
    "    Args:\n",
    "        data: List of text samples\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping words to IDs\n",
    "    \"\"\"\n",
    "    word_counts = {}\n",
    "    for text in data:\n",
    "        text_no_diac = araby.strip_diacritics(text)\n",
    "        words = araby.tokenize(text_no_diac)\n",
    "        for word in words:\n",
    "            if word.strip():  \n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    \n",
    "    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    word_to_id = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for idx, (word, _) in enumerate(sorted_words):\n",
    "        word_to_id[word] = idx + 2\n",
    "    \n",
    "    return word_to_id\n",
    "\n",
    "word_to_id = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Read train and val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:07.148372Z",
     "iopub.status.busy": "2025-12-09T12:08:07.148193Z",
     "iopub.status.idle": "2025-12-09T12:08:07.355684Z",
     "shell.execute_reply": "2025-12-09T12:08:07.354637Z",
     "shell.execute_reply.started": "2025-12-09T12:08:07.148358Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "val_data = []\n",
    "with open(TRAIN_PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        train_data.append(line.strip())\n",
    "with open(VAL_PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        val_data.append(line.strip())\n",
    "print(len(train_data))\n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:07.358731Z",
     "iopub.status.busy": "2025-12-09T12:08:07.358293Z",
     "iopub.status.idle": "2025-12-09T12:08:07.369323Z",
     "shell.execute_reply": "2025-12-09T12:08:07.368358Z",
     "shell.execute_reply.started": "2025-12-09T12:08:07.358702Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_arabic_text(text):\n",
    "    \"\"\"\n",
    "    Clean text to keep only Arabic letters, diacritics, and spaces\n",
    "    \"\"\"\n",
    "    allowed_chars = arabic_letters.union(diacritics, {' ', '\\t', '\\n'})\n",
    "    \n",
    "    cleaned_text = ''.join(char for char in text if char in allowed_chars)\n",
    "    \n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def split_sentences(sentences, window_size=window_size):\n",
    "    all_segments = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = araby.tokenize(sentence)\n",
    "        current_segment = []\n",
    "        current_len = 0\n",
    "        \n",
    "        for word in words:\n",
    "            word_len = len(word)\n",
    "            add_space = 1 if current_segment else 0\n",
    "            \n",
    "            if current_len + word_len + add_space <= window_size:\n",
    "                current_segment.append(word)\n",
    "                current_len += word_len + add_space\n",
    "            else:\n",
    "                if current_segment:\n",
    "                    all_segments.append(\" \".join(current_segment))\n",
    "                \n",
    "                current_segment = [word]\n",
    "                current_len = word_len\n",
    "        \n",
    "        if current_segment:\n",
    "            all_segments.append(\" \".join(current_segment))\n",
    "\n",
    "    return all_segments\n",
    "\n",
    "\n",
    "def sentence_tokeniz(sentences):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        subsentences = araby.sentence_tokenize(sentence)\n",
    "        tokenized_sentences.extend(subsentences)\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:07.371160Z",
     "iopub.status.busy": "2025-12-09T12:08:07.370385Z",
     "iopub.status.idle": "2025-12-09T12:08:19.695530Z",
     "shell.execute_reply": "2025-12-09T12:08:19.694827Z",
     "shell.execute_reply.started": "2025-12-09T12:08:07.371132Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building word vocabulary...\n",
      "Word vocabulary size: 105864\n"
     ]
    }
   ],
   "source": [
    "train_data = sentence_tokeniz(train_data)\n",
    "val_data = sentence_tokeniz(val_data)\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i] = clean_arabic_text(train_data[i])\n",
    "for i in range(len(val_data)):\n",
    "    val_data[i] = clean_arabic_text(val_data[i])\n",
    "\n",
    "train_data = split_sentences(train_data, window_size)\n",
    "val_data = split_sentences(val_data, window_size)\n",
    "\n",
    "print(\"Building word vocabulary...\")\n",
    "word_to_id = build_word_vocabulary(train_data)\n",
    "print(f\"Word vocabulary size: {len(word_to_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:19.696469Z",
     "iopub.status.busy": "2025-12-09T12:08:19.696290Z",
     "iopub.status.idle": "2025-12-09T12:08:19.701433Z",
     "shell.execute_reply": "2025-12-09T12:08:19.700670Z",
     "shell.execute_reply.started": "2025-12-09T12:08:19.696454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def is_diacritic(ch):\n",
    "    return unicodedata.combining(ch) != 0\n",
    "\n",
    "def extract_base_and_diacritics(text):\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    bases = []\n",
    "    diacs = []\n",
    "    current_base = None\n",
    "    current_diac = ''\n",
    "    for ch in text:\n",
    "        if is_diacritic(ch):\n",
    "            current_diac += ch\n",
    "        else:\n",
    "            if current_base is not None:\n",
    "                bases.append(current_base)\n",
    "                diacs.append(current_diac)\n",
    "            current_base = ch\n",
    "            current_diac = ''\n",
    "    if current_base is not None:\n",
    "        bases.append(current_base)\n",
    "        diacs.append(current_diac)\n",
    "    return bases, diacs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:19.702448Z",
     "iopub.status.busy": "2025-12-09T12:08:19.702199Z",
     "iopub.status.idle": "2025-12-09T12:08:19.719492Z",
     "shell.execute_reply": "2025-12-09T12:08:19.718798Z",
     "shell.execute_reply.started": "2025-12-09T12:08:19.702425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_char_and_word_features(text, word_to_id):\n",
    "    \"\"\"\n",
    "    Extract both character-level and word-level features from text\n",
    "    \n",
    "    Args:\n",
    "        text: Input text with diacritics\n",
    "        word_to_id: Dictionary mapping words to IDs\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (char_ids, diacritic_ids, word_ids, word_positions)\n",
    "        - char_ids: List of character IDs\n",
    "        - diacritic_ids: List of diacritic IDs for each character\n",
    "        - word_ids: List of word IDs aligned with characters\n",
    "        - word_positions: List indicating position in word (0=not end, 1=end, 2=space)\n",
    "    \"\"\"\n",
    "    bases, diacs = extract_base_and_diacritics(text)\n",
    "    \n",
    "    UNKNOWN_DIACRITIC_ID = diacritics_to_id.get('', len(diacritics_to_id) - 1)\n",
    "\n",
    "    char_ids = [char_to_id.get(c, char_to_id['UNK']) for c in bases]\n",
    "    diacritic_ids = [diacritics_to_id.get(d, UNKNOWN_DIACRITIC_ID) for d in diacs]\n",
    "    \n",
    "    text_no_diac = araby.strip_diacritics(text)\n",
    "    words = araby.tokenize(text_no_diac)\n",
    "    \n",
    "    word_ids = []\n",
    "    word_positions = [] # 0=not end, 1=end, 2=space\n",
    "    \n",
    "    char_idx = 0\n",
    "    for word in words:\n",
    "        if not word.strip():\n",
    "            continue\n",
    "        \n",
    "        word_id = word_to_id.get(word, word_to_id['<UNK>'])\n",
    "        word_len = len(word)\n",
    "        \n",
    "        for i in range(word_len):\n",
    "            if char_idx < len(char_ids):\n",
    "                word_ids.append(word_id)\n",
    "                if i == word_len - 1:\n",
    "                    word_positions.append(1)\n",
    "                else:\n",
    "                    word_positions.append(0)\n",
    "                char_idx += 1\n",
    "        \n",
    "        if char_idx < len(char_ids) and bases[char_idx] == ' ':\n",
    "            word_ids.append(0)  # for padding\n",
    "            word_positions.append(2)\n",
    "            char_idx += 1\n",
    "    \n",
    "    while len(word_ids) < len(char_ids):\n",
    "        word_ids.append(0)\n",
    "        word_positions.append(2)\n",
    "    \n",
    "    return char_ids, diacritic_ids, word_ids, word_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:19.720386Z",
     "iopub.status.busy": "2025-12-09T12:08:19.720195Z",
     "iopub.status.idle": "2025-12-09T12:08:19.768543Z",
     "shell.execute_reply": "2025-12-09T12:08:19.767706Z",
     "shell.execute_reply.started": "2025-12-09T12:08:19.720371Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(char_to_id, open(CHAR_TO_ID_PATH, 'wb'))\n",
    "pickle.dump(word_to_id, open(WORD_TO_ID_PATH, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prepare data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:19.769684Z",
     "iopub.status.busy": "2025-12-09T12:08:19.769471Z",
     "iopub.status.idle": "2025-12-09T12:08:34.227769Z",
     "shell.execute_reply": "2025-12-09T12:08:34.227076Z",
     "shell.execute_reply.started": "2025-12-09T12:08:19.769669Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting character and word-level features from training data...\n",
      "Training samples: 172467\n"
     ]
    }
   ],
   "source": [
    "x_train_char_raw = []\n",
    "y_train_raw = []\n",
    "x_train_word_raw = []\n",
    "x_train_word_position_raw = []\n",
    "\n",
    "UNKNOWN_DIACRITIC_ID = diacritics_to_id.get('', len(diacritics_to_id) - 1)\n",
    "\n",
    "print(\"Extracting character and word-level features from training data...\")\n",
    "for text in train_data:\n",
    "    char_ids, diacritic_ids, word_ids, word_positions = extract_char_and_word_features(text, word_to_id)\n",
    "    \n",
    "    x_train_char_raw.append(char_ids)\n",
    "    y_train_raw.append(diacritic_ids)\n",
    "    x_train_word_raw.append(word_ids)\n",
    "    x_train_word_position_raw.append(word_positions)\n",
    "\n",
    "print(f\"Training samples: {len(x_train_char_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:34.228904Z",
     "iopub.status.busy": "2025-12-09T12:08:34.228667Z",
     "iopub.status.idle": "2025-12-09T12:08:38.082315Z",
     "shell.execute_reply": "2025-12-09T12:08:38.081490Z",
     "shell.execute_reply.started": "2025-12-09T12:08:34.228887Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_char shape: (172467, 607)\n",
      "x_train_word shape: (172467, 607)\n",
      "x_train_position shape: (172467, 607)\n",
      "y_train shape: (172467, 607)\n"
     ]
    }
   ],
   "source": [
    "PAD_DIACRITIC_ID = diacritics_to_id.get('', 0)\n",
    "\n",
    "x_train_char = tf.keras.preprocessing.sequence.pad_sequences(x_train_char_raw, padding='post', value=0)\n",
    "x_train_word = tf.keras.preprocessing.sequence.pad_sequences(x_train_word_raw, padding='post', value=0)\n",
    "x_train_position = tf.keras.preprocessing.sequence.pad_sequences(x_train_word_position_raw, padding='post', value=2)\n",
    "y_train = tf.keras.preprocessing.sequence.pad_sequences(y_train_raw, padding='post', value=PAD_DIACRITIC_ID)\n",
    "\n",
    "print(f\"x_train_char shape: {x_train_char.shape}\")\n",
    "print(f\"x_train_word shape: {x_train_word.shape}\")\n",
    "print(f\"x_train_position shape: {x_train_position.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:38.083486Z",
     "iopub.status.busy": "2025-12-09T12:08:38.083198Z",
     "iopub.status.idle": "2025-12-09T12:08:39.363472Z",
     "shell.execute_reply": "2025-12-09T12:08:39.362740Z",
     "shell.execute_reply.started": "2025-12-09T12:08:38.083460Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting character and word-level features from validation data...\n",
      "Validation samples: 8332\n"
     ]
    }
   ],
   "source": [
    "x_val_char_raw = []\n",
    "y_val_raw = []\n",
    "x_val_word_raw = []\n",
    "x_val_word_position_raw = []\n",
    "\n",
    "print(\"Extracting character and word-level features from validation data...\")\n",
    "for text in val_data:\n",
    "    char_ids, diacritic_ids, word_ids, word_positions = extract_char_and_word_features(text, word_to_id)\n",
    "    \n",
    "    x_val_char_raw.append(char_ids)\n",
    "    y_val_raw.append(diacritic_ids)\n",
    "    x_val_word_raw.append(word_ids)\n",
    "    x_val_word_position_raw.append(word_positions)\n",
    "\n",
    "print(f\"Validation samples: {len(x_val_char_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:39.365955Z",
     "iopub.status.busy": "2025-12-09T12:08:39.365756Z",
     "iopub.status.idle": "2025-12-09T12:08:39.564561Z",
     "shell.execute_reply": "2025-12-09T12:08:39.563760Z",
     "shell.execute_reply.started": "2025-12-09T12:08:39.365937Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_val_char shape: (8332, 597)\n",
      "x_val_word shape: (8332, 597)\n",
      "x_val_position shape: (8332, 597)\n",
      "y_val shape: (8332, 597)\n"
     ]
    }
   ],
   "source": [
    "x_val_char = tf.keras.preprocessing.sequence.pad_sequences(x_val_char_raw, padding='post', value=0)\n",
    "x_val_word = tf.keras.preprocessing.sequence.pad_sequences(x_val_word_raw, padding='post', value=0)\n",
    "x_val_position = tf.keras.preprocessing.sequence.pad_sequences(x_val_word_position_raw, padding='post', value=2)\n",
    "y_val = tf.keras.preprocessing.sequence.pad_sequences(y_val_raw, padding='post', value=PAD_DIACRITIC_ID)\n",
    "\n",
    "print(f\"x_val_char shape: {x_val_char.shape}\")\n",
    "print(f\"x_val_word shape: {x_val_word.shape}\")\n",
    "print(f\"x_val_position shape: {x_val_position.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:39.565515Z",
     "iopub.status.busy": "2025-12-09T12:08:39.565300Z",
     "iopub.status.idle": "2025-12-09T12:08:39.572454Z",
     "shell.execute_reply": "2025-12-09T12:08:39.571890Z",
     "shell.execute_reply.started": "2025-12-09T12:08:39.565498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, pad_id=0, space_id=None, name='DER', **kwargs):\n",
    "        super(DERMetric, self).__init__(name=name, **kwargs)\n",
    "        self.pad_id = pad_id\n",
    "        self.space_id = space_id\n",
    "        self.total = self.add_weight(name='total', initializer='zeros')\n",
    "        self.errors = self.add_weight(name='errors', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        y_true: shape (batch_size, seq_len)\n",
    "        y_pred: shape (batch_size, seq_len, num_classes)\n",
    "        \"\"\"\n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1, output_type=tf.int32)\n",
    "\n",
    "        mask = tf.not_equal(y_true, self.pad_id)\n",
    "        if self.space_id is not None:\n",
    "            mask = tf.logical_and(mask, tf.not_equal(y_true, self.space_id))\n",
    "\n",
    "        correct = tf.equal(y_true, y_pred_labels)\n",
    "        correct = tf.logical_and(correct, mask)\n",
    "\n",
    "        batch_errors = tf.reduce_sum(tf.cast(~correct, tf.float32))\n",
    "        batch_total = tf.reduce_sum(tf.cast(mask, tf.float32))\n",
    "\n",
    "        self.errors.assign_add(batch_errors)\n",
    "        self.total.assign_add(batch_total)\n",
    "\n",
    "    def result(self):\n",
    "        return (self.errors / self.total) * 100  # DER in %\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.errors.assign(0)\n",
    "        self.total.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:39.573370Z",
     "iopub.status.busy": "2025-12-09T12:08:39.573131Z",
     "iopub.status.idle": "2025-12-09T12:08:39.592729Z",
     "shell.execute_reply": "2025-12-09T12:08:39.592141Z",
     "shell.execute_reply.started": "2025-12-09T12:08:39.573350Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_der_by_position(x_val_char, y_true, y_pred, char_to_id):\n",
    "    \"\"\"\n",
    "    Calculate DER separately for last characters and non-last characters in words\n",
    "    \n",
    "    Args:\n",
    "        x_val_char: Character sequences (samples x sequence_length)\n",
    "        y_true: Ground truth diacritic labels (samples x sequence_length)\n",
    "        y_pred: Predicted diacritic labels (samples x sequence_length)\n",
    "        char_to_id: Dictionary mapping characters to IDs\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (DER_non_last, DER_last, overall_DER)\n",
    "    \"\"\"\n",
    "    space_id = char_to_id.get(' ', char_to_id.get('UNK'))\n",
    "    pad_id = char_to_id.get('<PAD>', 0)\n",
    "    \n",
    "    non_last_errors = 0\n",
    "    non_last_total = 0\n",
    "    last_errors = 0\n",
    "    last_total = 0\n",
    "    \n",
    "    for char_seq, y_true_seq, y_pred_seq in zip(x_val_char, y_true, y_pred):\n",
    "        valid_mask = char_seq != pad_id\n",
    "        valid_indices = np.where(valid_mask)[0]\n",
    "        \n",
    "        if len(valid_indices) == 0:\n",
    "            continue\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(valid_indices):\n",
    "            idx = valid_indices[i]\n",
    "            \n",
    "            if char_seq[idx] == space_id:\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            word_start = i\n",
    "            while i < len(valid_indices) and char_seq[valid_indices[i]] != space_id:\n",
    "                i += 1\n",
    "            word_end = i - 1\n",
    "            \n",
    "            for j in range(word_start, word_end + 1):\n",
    "                pos_idx = valid_indices[j]\n",
    "                \n",
    "                if y_true_seq[pos_idx] == PAD_DIACRITIC_ID:\n",
    "                    continue\n",
    "                \n",
    "                is_correct = (y_true_seq[pos_idx] == y_pred_seq[pos_idx])\n",
    "                \n",
    "                if j == word_end:\n",
    "                    last_total += 1\n",
    "                    if not is_correct:\n",
    "                        last_errors += 1\n",
    "                else:\n",
    "                    non_last_total += 1\n",
    "                    if not is_correct:\n",
    "                        non_last_errors += 1\n",
    "    \n",
    "    der_non_last = (non_last_errors / non_last_total * 100) if non_last_total > 0 else 0\n",
    "    der_last = (last_errors / last_total * 100) if last_total > 0 else 0\n",
    "    \n",
    "    total_errors = non_last_errors + last_errors\n",
    "    total_chars = non_last_total + last_total\n",
    "    der_overall = (total_errors / total_chars * 100) if total_chars > 0 else 0\n",
    "    \n",
    "    return der_non_last, der_last, der_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:39.593795Z",
     "iopub.status.busy": "2025-12-09T12:08:39.593564Z",
     "iopub.status.idle": "2025-12-09T12:08:39.615114Z",
     "shell.execute_reply": "2025-12-09T12:08:39.614434Z",
     "shell.execute_reply.started": "2025-12-09T12:08:39.593777Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, word_vocab_size, num_diacritics, pad_id):\n",
    "    char_input = Input(shape=(None,), name='char_input')\n",
    "    char_embedding = layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=128,\n",
    "        mask_zero=True,\n",
    "        name='char_embedding'\n",
    "    )(char_input)\n",
    "\n",
    "    word_input = Input(shape=(None,), name='word_input')\n",
    "    word_embedding = layers.Embedding(\n",
    "        input_dim=word_vocab_size,\n",
    "        output_dim=128,\n",
    "        mask_zero=True,\n",
    "        name='word_embedding'\n",
    "    )(word_input)\n",
    "\n",
    "    position_input = Input(shape=(None,), name='position_input')\n",
    "    position_embedding = layers.Embedding(\n",
    "        input_dim=3,\n",
    "        output_dim=16,\n",
    "        mask_zero=False,\n",
    "        name='position_embedding'\n",
    "    )(position_input)\n",
    "\n",
    "    combined = layers.Concatenate(name='feature_concat')([\n",
    "        char_embedding,\n",
    "        word_embedding,\n",
    "        position_embedding\n",
    "    ])\n",
    "\n",
    "    combined._keras_mask = char_embedding._keras_mask\n",
    "\n",
    "    bilstm1 = layers.Bidirectional(\n",
    "        layers.LSTM(\n",
    "            256,\n",
    "            return_sequences=True,\n",
    "            activation='tanh',\n",
    "            recurrent_activation='sigmoid'\n",
    "        ),\n",
    "        name='bilstm_1'\n",
    "    )(combined)\n",
    "\n",
    "\n",
    "    bilstm2 = layers.Bidirectional(\n",
    "        layers.LSTM(\n",
    "            256,\n",
    "            return_sequences=True,\n",
    "            activation='tanh',\n",
    "            recurrent_activation='sigmoid'\n",
    "        ),\n",
    "        name='bilstm_2'\n",
    "    )(bilstm1)\n",
    "\n",
    "\n",
    "    dense1 = layers.Dense(\n",
    "        256,\n",
    "        activation='relu',\n",
    "        name='dense_1'\n",
    "    )(bilstm2)\n",
    "\n",
    "\n",
    "    dense2 = layers.Dense(\n",
    "        256,\n",
    "        activation='relu',\n",
    "        name='dense_2'\n",
    "    )(dense1)\n",
    "\n",
    "    output = layers.Dense(\n",
    "        num_diacritics,\n",
    "        activation='softmax',\n",
    "        name='diacritic_output'\n",
    "    )(dense2)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[char_input, word_input, position_input],\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "    der_metric = DERMetric(pad_id=0, space_id=pad_id)\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[der_metric]\n",
    "    )\n",
    "\n",
    "    print(\"\\nModel Architecture:\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:39.616145Z",
     "iopub.status.busy": "2025-12-09T12:08:39.615874Z",
     "iopub.status.idle": "2025-12-09T12:08:39.838175Z",
     "shell.execute_reply": "2025-12-09T12:08:39.837566Z",
     "shell.execute_reply.started": "2025-12-09T12:08:39.616122Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ char_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_input      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ char_embedding      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,864</span> │ char_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_embedding      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">13,550,592</span> │ word_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_embedding  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span> │ position_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ feature_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ char_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ word_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ position_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ char_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bilstm_1            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,083,392</span> │ feature_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bilstm_2            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ bilstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ bilstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ diacritic_output    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,855</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ char_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_input      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ char_embedding      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │      \u001b[38;5;34m4,864\u001b[0m │ char_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_embedding      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │ \u001b[38;5;34m13,550,592\u001b[0m │ word_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_embedding  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)  │         \u001b[38;5;34m48\u001b[0m │ position_input[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ feature_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m272\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ char_embedding[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ word_embedding[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ position_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ char_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bilstm_1            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │  \u001b[38;5;34m1,083,392\u001b[0m │ feature_concat[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bilstm_2            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │  \u001b[38;5;34m1,574,912\u001b[0m │ bilstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m131,328\u001b[0m │ bilstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │     \u001b[38;5;34m65,792\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ diacritic_output    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)  │      \u001b[38;5;34m3,855\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,414,783</span> (62.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,414,783\u001b[0m (62.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,414,783</span> (62.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m16,414,783\u001b[0m (62.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(char_to_id),\n",
    "    word_vocab_size=len(word_to_id),\n",
    "    num_diacritics=len(diacritics_to_id),\n",
    "    pad_id=char_to_id.get('<PAD>', 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Callbacks for Training\n",
    "Configure early stopping and model checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:39.838900Z",
     "iopub.status.busy": "2025-12-09T12:08:39.838722Z",
     "iopub.status.idle": "2025-12-09T12:08:39.844273Z",
     "shell.execute_reply": "2025-12-09T12:08:39.843346Z",
     "shell.execute_reply.started": "2025-12-09T12:08:39.838886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_SAVE_DIR):\n",
    "    os.makedirs(MODEL_SAVE_DIR)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_DER',           \n",
    "    patience=3,                 \n",
    "    mode='min',                 \n",
    "    verbose=1,\n",
    "    restore_best_weights=True    \n",
    ")\n",
    "\n",
    "checkpoint_path = MODEL_WEIGHTS_PATH\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_DER',           \n",
    "    mode='min',                  \n",
    "    save_best_only=True,    \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_DER',\n",
    "    factor=0.5,                  \n",
    "    patience=2,                  \n",
    "    mode='min',\n",
    "    verbose=1,\n",
    "    min_lr=1e-7                \n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, model_checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model with Validation and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:08:39.845193Z",
     "iopub.status.busy": "2025-12-09T12:08:39.844939Z",
     "iopub.status.idle": "2025-12-09T12:49:36.678442Z",
     "shell.execute_reply": "2025-12-09T12:49:36.677623Z",
     "shell.execute_reply.started": "2025-12-09T12:08:39.845175Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2695/2695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - DER: 4.2778 - loss: 0.1674\n",
      "Epoch 1: val_DER improved from inf to 3.19949, saving model to ./models/best_weights.keras\n",
      "\u001b[1m2695/2695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 129ms/step - DER: 4.2774 - loss: 0.1673 - val_DER: 3.1995 - val_loss: 0.0085 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m2695/2695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - DER: 3.0416 - loss: 0.0065\n",
      "Epoch 2: val_DER improved from 3.19949 to 3.14476, saving model to ./models/best_weights.keras\n",
      "\u001b[1m2695/2695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 130ms/step - DER: 3.0416 - loss: 0.0065 - val_DER: 3.1448 - val_loss: 0.0072 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m2695/2695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - DER: 3.0003 - loss: 0.0048\n",
      "Epoch 3: val_DER improved from 3.14476 to 3.12581, saving model to ./models/best_weights.keras\n",
      "\u001b[1m2695/2695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 130ms/step - DER: 3.0003 - loss: 0.0048 - val_DER: 3.1258 - val_loss: 0.0070 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m2695/2695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - DER: 2.9704 - loss: 0.0038\n",
      "Epoch 4: val_DER improved from 3.12581 to 3.12287, saving model to ./models/best_weights.keras\n",
      "\u001b[1m2695/2695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 130ms/step - DER: 2.9704 - loss: 0.0038 - val_DER: 3.1229 - val_loss: 0.0074 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m2695/2695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - DER: 2.9738 - loss: 0.0030\n",
      "Epoch 5: val_DER did not improve from 3.12287\n",
      "\u001b[1m2695/2695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 130ms/step - DER: 2.9738 - loss: 0.0030 - val_DER: 3.1362 - val_loss: 0.0076 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m2695/2695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - DER: 2.9430 - loss: 0.0024\n",
      "Epoch 6: val_DER did not improve from 3.12287\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m2695/2695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 130ms/step - DER: 2.9430 - loss: 0.0024 - val_DER: 3.1248 - val_loss: 0.0081 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m2695/2695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - DER: 2.9216 - loss: 0.0016\n",
      "Epoch 7: val_DER did not improve from 3.12287\n",
      "\u001b[1m2695/2695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 130ms/step - DER: 2.9216 - loss: 0.0016 - val_DER: 3.1268 - val_loss: 0.0100 - learning_rate: 5.0000e-04\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(\n",
    "        {'char_input': x_train_char, 'word_input': x_train_word, 'position_input': x_train_position},\n",
    "        y_train,\n",
    "        validation_data=(\n",
    "            {'char_input': x_val_char, 'word_input': x_val_word, 'position_input': x_val_position},\n",
    "            y_val\n",
    "        ),\n",
    "        epochs=10,                    \n",
    "        batch_size=64,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:49:36.680361Z",
     "iopub.status.busy": "2025-12-09T12:49:36.680031Z",
     "iopub.status.idle": "2025-12-09T12:49:37.533066Z",
     "shell.execute_reply": "2025-12-09T12:49:37.532415Z",
     "shell.execute_reply.started": "2025-12-09T12:49:36.680343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save(\"BiLSTM_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Best Model \n",
    "Load the best model saved during training locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T12:49:37.534016Z",
     "iopub.status.busy": "2025-12-09T12:49:37.533817Z",
     "iopub.status.idle": "2025-12-09T12:49:37.551383Z",
     "shell.execute_reply": "2025-12-09T12:49:37.550345Z",
     "shell.execute_reply.started": "2025-12-09T12:49:37.533999Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File not found: filepath=./models/BiLSTM_model.keras. Please ensure the file is an accessible `.keras` zip file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47/2079880280.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mMODEL_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./models/BiLSTM_model.keras'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"DERMetric\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDERMetric\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BiLSTM_weights.weights.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n\u001b[1;32m    199\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0;34mf\"File not found: filepath={filepath}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;34m\"Please ensure the file is an accessible `.keras` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: File not found: filepath=./models/BiLSTM_model.keras. Please ensure the file is an accessible `.keras` zip file."
     ]
    }
   ],
   "source": [
    "MODEL_PATH = './models/BiLSTM_model.keras'\n",
    "model = tf.keras.models.load_model(MODEL_PATH, custom_objects={\"DERMetric\": DERMetric})\n",
    "model.save_weights(\"BiLSTM_weights.weights.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Test Val</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-09T12:49:37.551991Z",
     "iopub.status.idle": "2025-12-09T12:49:37.552348Z",
     "shell.execute_reply": "2025-12-09T12:49:37.552195Z",
     "shell.execute_reply.started": "2025-12-09T12:49:37.552176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict({'char_input': x_val_char, 'word_input': x_val_word, 'position_input': x_val_position})\n",
    "y_pred_classes = np.argmax(y_pred, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-09T12:49:37.553291Z",
     "iopub.status.idle": "2025-12-09T12:49:37.553584Z",
     "shell.execute_reply": "2025-12-09T12:49:37.553434Z",
     "shell.execute_reply.started": "2025-12-09T12:49:37.553420Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_true = y_val\n",
    "\n",
    "accuracy = accuracy_score(y_true.flatten(), y_pred_classes.flatten())\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-09T12:49:37.554656Z",
     "iopub.status.idle": "2025-12-09T12:49:37.554965Z",
     "shell.execute_reply": "2025-12-09T12:49:37.554822Z",
     "shell.execute_reply.started": "2025-12-09T12:49:37.554808Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "der_non_last, der_last, der_overall = calculate_der_by_position(x_val_char, y_true, y_pred_classes, char_to_id)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DER Analysis by Character Position in Words\")\n",
    "print(\"=\"*60)\n",
    "print(f\"DER for non-last characters: {der_non_last:.2f}%\")\n",
    "print(f\"DER for last characters:     {der_last:.2f}%\")\n",
    "print(f\"Overall DER:                 {der_overall:.2f}%\")\n",
    "print(f\"\\nAccuracy for non-last characters: {100 - der_non_last:.2f}%\")\n",
    "print(f\"Accuracy for last characters:     {100 - der_last:.2f}%\")\n",
    "print(f\"Total Accuracy: {100 - der_overall:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
