{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ea3b3b",
   "metadata": {},
   "source": [
    "<h1> Arabic letters and diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2d1196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import pyarabic.araby as araby\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input, Model\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58710ed7",
   "metadata": {},
   "source": [
    "<h2> Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92a78fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 1000\n",
    "\n",
    "ARABIC_LETTERS_PATH = './utils/arabic_letters.pickle'\n",
    "DIACRITICS_PATH = './utils/diacritics.pickle'\n",
    "DIACRITICS_TO_ID_PATH = './utils/diacritic2id.pickle'\n",
    "CHAR_TO_ID_PATH = './utils/char2id.pickle'\n",
    "WORD_TO_ID_PATH = './utils/word2id.pickle'\n",
    "\n",
    "TEST_PATH = './data/dataset_no_diacritics.txt'\n",
    "\n",
    "MODEL_WEIGHTS_PATH = './models/best_weights.ckpt'\n",
    "\n",
    "MODEL_PATH = './models/BiLSTM_V2_model.keras'\n",
    "\n",
    "GOLD_PATH = './data/sample_test_set_gold.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6e6219",
   "metadata": {},
   "source": [
    "## GPU Configuration for Kaggle\n",
    "Check GPU availability and configure TensorFlow to use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c1a72dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "No GPU found. Training will use CPU.\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        print(f\"GPU detected: {gpus}\")\n",
    "        print(f\"GPU Name: {tf.test.gpu_device_name()}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found. Training will use CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f7559f",
   "metadata": {},
   "source": [
    "## Load Dictionaries and Create Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "790bb5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_letters = []\n",
    "diacritics = []\n",
    "diacritics_to_id = {}\n",
    "char_to_id = {}\n",
    "word_to_id = {}\n",
    "\n",
    "with open(ARABIC_LETTERS_PATH, 'rb') as f:\n",
    "    arabic_letters = pickle.load(f)\n",
    "with open(DIACRITICS_PATH, 'rb') as f:\n",
    "    diacritics = pickle.load(f)\n",
    "with open(DIACRITICS_TO_ID_PATH, 'rb') as f:\n",
    "    diacritics_to_id = pickle.load(f)\n",
    "with open(CHAR_TO_ID_PATH, 'rb') as f:\n",
    "    char_to_id = pickle.load(f)\n",
    "with open(WORD_TO_ID_PATH, 'rb') as f:\n",
    "    word_to_id = pickle.load(f)\n",
    "\n",
    "\n",
    "gold = pd.read_csv(GOLD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969307a3",
   "metadata": {},
   "source": [
    "<h2> Read test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "edd7cbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2461\n"
     ]
    }
   ],
   "source": [
    "test_data = []\n",
    "with open(TEST_PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        test_data.append(line.strip())\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccd1ca2",
   "metadata": {},
   "source": [
    "<h2> Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab11cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_arabic_text(text):\n",
    "    \"\"\"\n",
    "    Clean text to keep only Arabic letters, diacritics, and spaces\n",
    "    \"\"\"\n",
    "    allowed_chars = arabic_letters.union(diacritics, {' ', '\\t', '\\n'})\n",
    "    \n",
    "    cleaned_text = ''.join(char for char in text if char in allowed_chars)\n",
    "    \n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def split_sentences(sentences, window_size=window_size):\n",
    "    all_segments = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = araby.tokenize(sentence)\n",
    "        current_segment = []\n",
    "        current_len = 0\n",
    "        \n",
    "        for word in words:\n",
    "            word_len = len(word)\n",
    "            add_space = 1 if current_segment else 0\n",
    "            \n",
    "            if current_len + word_len + add_space <= window_size:\n",
    "                current_segment.append(word)\n",
    "                current_len += word_len + add_space\n",
    "            else:\n",
    "                if current_segment:\n",
    "                    all_segments.append(\" \".join(current_segment))\n",
    "                \n",
    "                current_segment = [word]\n",
    "                current_len = word_len\n",
    "        \n",
    "        if current_segment:\n",
    "            all_segments.append(\" \".join(current_segment))\n",
    "\n",
    "    return all_segments\n",
    "\n",
    "\n",
    "def sentence_tokeniz(sentences):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        subsentences = araby.sentence_tokenize(sentence)\n",
    "        tokenized_sentences.extend(subsentences)\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9014bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = sentence_tokeniz(test_data)\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    test_data[i] = clean_arabic_text(test_data[i])\n",
    "\n",
    "test_data = split_sentences(test_data, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25ce1734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def is_diacritic(ch):\n",
    "    return unicodedata.combining(ch) != 0\n",
    "\n",
    "def extract_base_and_diacritics(text):\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    bases = []\n",
    "    diacs = []\n",
    "    current_base = None\n",
    "    current_diac = ''\n",
    "    for ch in text:\n",
    "        if is_diacritic(ch):\n",
    "            current_diac += ch\n",
    "        else:\n",
    "            if current_base is not None:\n",
    "                bases.append(current_base)\n",
    "                diacs.append(current_diac)\n",
    "            current_base = ch\n",
    "            current_diac = ''\n",
    "    if current_base is not None:\n",
    "        bases.append(current_base)\n",
    "        diacs.append(current_diac)\n",
    "    return bases, diacs\n",
    "\n",
    "def extract_char_and_word_features(text, word_to_id):\n",
    "    \"\"\"\n",
    "    Extract both character-level and word-level features from text (without diacritics)\n",
    "    \n",
    "    Args:\n",
    "        text: Input text WITHOUT diacritics\n",
    "        word_to_id: Dictionary mapping words to IDs\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (char_ids, word_ids, word_positions)\n",
    "        - char_ids: List of character IDs\n",
    "        - word_ids: List of word IDs aligned with characters\n",
    "        - word_positions: List indicating position in word (0=not end, 1=end, 2=space)\n",
    "    \"\"\"\n",
    "    # For prediction, text has no diacritics, so we create dummy diacritics\n",
    "    # to match the training format\n",
    "    text_no_diac = araby.strip_diacritics(text)\n",
    "    \n",
    "    # Create a version with spaces to match training\n",
    "    bases = []\n",
    "    for ch in text_no_diac:\n",
    "        bases.append(ch)\n",
    "    \n",
    "    UNKNOWN_DIACRITIC_ID = diacritics_to_id.get('', len(diacritics_to_id) - 1)\n",
    "\n",
    "    char_ids = [char_to_id.get(c, char_to_id['UNK']) for c in bases]\n",
    "    \n",
    "    words = araby.tokenize(text_no_diac)\n",
    "    \n",
    "    word_ids = []\n",
    "    word_positions = [] # 0=not end, 1=end, 2=space\n",
    "    \n",
    "    char_idx = 0\n",
    "    for word in words:\n",
    "        if not word.strip():\n",
    "            continue\n",
    "        \n",
    "        word_id = word_to_id.get(word, word_to_id['<UNK>'])\n",
    "        word_len = len(word)\n",
    "        \n",
    "        for i in range(word_len):\n",
    "            if char_idx < len(char_ids):\n",
    "                word_ids.append(word_id)\n",
    "                if i == word_len - 1:\n",
    "                    word_positions.append(1)\n",
    "                else:\n",
    "                    word_positions.append(0)\n",
    "                char_idx += 1\n",
    "        \n",
    "        # Handle space after word\n",
    "        if char_idx < len(char_ids) and bases[char_idx] == ' ':\n",
    "            word_ids.append(0)  # padding for space\n",
    "            word_positions.append(2)\n",
    "            char_idx += 1\n",
    "    \n",
    "    # Fill remaining with padding\n",
    "    while len(word_ids) < len(char_ids):\n",
    "        word_ids.append(0)\n",
    "        word_positions.append(2)\n",
    "    \n",
    "    return char_ids, word_ids, word_positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a885f12a",
   "metadata": {},
   "source": [
    "<h2> Prepare data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b385343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting character and word-level features from testing data...\n",
      "Testing samples: 5735\n"
     ]
    }
   ],
   "source": [
    "x_test_char_raw = []\n",
    "x_test_word_raw = []\n",
    "x_test_word_position_raw = []\n",
    "\n",
    "print(\"Extracting character and word-level features from testing data...\")\n",
    "for text in test_data:\n",
    "    char_ids, word_ids, word_positions = extract_char_and_word_features(text, word_to_id)\n",
    "    \n",
    "    x_test_char_raw.append(char_ids)\n",
    "    x_test_word_raw.append(word_ids)\n",
    "    x_test_word_position_raw.append(word_positions)\n",
    "\n",
    "print(f\"Testing samples: {len(x_test_char_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d846c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test_char shape: (5735, 999)\n",
      "x_test_word shape: (5735, 999)\n",
      "x_test_position shape: (5735, 999)\n"
     ]
    }
   ],
   "source": [
    "x_test_char = tf.keras.preprocessing.sequence.pad_sequences(x_test_char_raw, padding='post', value=0)\n",
    "x_test_word = tf.keras.preprocessing.sequence.pad_sequences(x_test_word_raw, padding='post', value=0)\n",
    "x_test_position = tf.keras.preprocessing.sequence.pad_sequences(x_test_word_position_raw, padding='post', value=2)\n",
    "\n",
    "print(f\"x_test_char shape: {x_test_char.shape}\")\n",
    "print(f\"x_test_word shape: {x_test_word.shape}\")\n",
    "print(f\"x_test_position shape: {x_test_position.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f9badf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, pad_id=0, space_id=None, name='DER', **kwargs):\n",
    "        super(DERMetric, self).__init__(name=name, **kwargs)\n",
    "        self.pad_id = pad_id\n",
    "        self.space_id = space_id\n",
    "        self.total = self.add_weight(name='total', initializer='zeros')\n",
    "        self.errors = self.add_weight(name='errors', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        y_true: shape (batch_size, seq_len)\n",
    "        y_pred: shape (batch_size, seq_len, num_classes)\n",
    "        \"\"\"\n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1, output_type=tf.int32)\n",
    "\n",
    "        mask = tf.not_equal(y_true, self.pad_id)\n",
    "        if self.space_id is not None:\n",
    "            mask = tf.logical_and(mask, tf.not_equal(y_true, self.space_id))\n",
    "\n",
    "        correct = tf.equal(y_true, y_pred_labels)\n",
    "        correct = tf.logical_and(correct, mask)\n",
    "\n",
    "        batch_errors = tf.reduce_sum(tf.cast(~correct, tf.float32))\n",
    "        batch_total = tf.reduce_sum(tf.cast(mask, tf.float32))\n",
    "\n",
    "        self.errors.assign_add(batch_errors)\n",
    "        self.total.assign_add(batch_total)\n",
    "\n",
    "    def result(self):\n",
    "        return (self.errors / self.total) * 100  # DER in %\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.errors.assign(0)\n",
    "        self.total.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84e5edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_der_by_position(x_val_char, y_true, y_pred, char_to_id):\n",
    "    \"\"\"\n",
    "    Calculate DER separately for last characters and non-last characters in words\n",
    "    \n",
    "    Args:\n",
    "        x_val_char: Character sequences (samples x sequence_length)\n",
    "        y_true: Ground truth diacritic labels (samples x sequence_length)\n",
    "        y_pred: Predicted diacritic labels (samples x sequence_length)\n",
    "        char_to_id: Dictionary mapping characters to IDs\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (DER_non_last, DER_last, overall_DER)\n",
    "    \"\"\"\n",
    "    PAD_DIACRITIC_ID = diacritics_to_id.get('', 0)\n",
    "\n",
    "    space_id = char_to_id.get(' ', char_to_id.get('UNK'))\n",
    "    pad_id = char_to_id.get('<PAD>', 0)\n",
    "    \n",
    "    non_last_errors = 0\n",
    "    non_last_total = 0\n",
    "    last_errors = 0\n",
    "    last_total = 0\n",
    "    \n",
    "    for char_seq, y_true_seq, y_pred_seq in zip(x_val_char, y_true, y_pred):\n",
    "        valid_mask = char_seq != pad_id\n",
    "        valid_indices = np.where(valid_mask)[0]\n",
    "        \n",
    "        if len(valid_indices) == 0:\n",
    "            continue\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(valid_indices):\n",
    "            idx = valid_indices[i]\n",
    "            \n",
    "            if char_seq[idx] == space_id:\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            word_start = i\n",
    "            while i < len(valid_indices) and char_seq[valid_indices[i]] != space_id:\n",
    "                i += 1\n",
    "            word_end = i - 1\n",
    "            \n",
    "            for j in range(word_start, word_end + 1):\n",
    "                pos_idx = valid_indices[j]\n",
    "                \n",
    "                if y_true_seq[pos_idx] == PAD_DIACRITIC_ID:\n",
    "                    continue\n",
    "                \n",
    "                is_correct = (y_true_seq[pos_idx] == y_pred_seq[pos_idx])\n",
    "                \n",
    "                if j == word_end:\n",
    "                    last_total += 1\n",
    "                    if not is_correct:\n",
    "                        last_errors += 1\n",
    "                else:\n",
    "                    non_last_total += 1\n",
    "                    if not is_correct:\n",
    "                        non_last_errors += 1\n",
    "    \n",
    "    der_non_last = (non_last_errors / non_last_total * 100) if non_last_total > 0 else 0\n",
    "    der_last = (last_errors / last_total * 100) if last_total > 0 else 0\n",
    "    \n",
    "    total_errors = non_last_errors + last_errors\n",
    "    total_chars = non_last_total + last_total\n",
    "    der_overall = (total_errors / total_chars * 100) if total_chars > 0 else 0\n",
    "    \n",
    "    return der_non_last, der_last, der_overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7d9332",
   "metadata": {},
   "source": [
    "## Load and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73f75dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, word_vocab_size, num_diacritics, pad_id):\n",
    "    char_input = Input(shape=(None,), name='char_input')\n",
    "    char_embedding = layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=128,\n",
    "        mask_zero=True,\n",
    "        name='char_embedding'\n",
    "    )(char_input)\n",
    "\n",
    "    word_input = Input(shape=(None,), name='word_input')\n",
    "    word_embedding = layers.Embedding(\n",
    "        input_dim=word_vocab_size,\n",
    "        output_dim=128,\n",
    "        mask_zero=True,\n",
    "        name='word_embedding'\n",
    "    )(word_input)\n",
    "\n",
    "    position_input = Input(shape=(None,), name='position_input')\n",
    "    position_embedding = layers.Embedding(\n",
    "        input_dim=3,\n",
    "        output_dim=16,\n",
    "        mask_zero=False,\n",
    "        name='position_embedding'\n",
    "    )(position_input)\n",
    "\n",
    "    combined = layers.Concatenate(name='feature_concat')([\n",
    "        char_embedding,\n",
    "        word_embedding,\n",
    "        position_embedding\n",
    "    ])\n",
    "\n",
    "    combined._keras_mask = char_embedding._keras_mask\n",
    "\n",
    "    lstm_out = layers.Bidirectional(\n",
    "        layers.LSTM(\n",
    "            256,\n",
    "            return_sequences=True,\n",
    "            activation='tanh',\n",
    "            recurrent_activation='sigmoid'\n",
    "        ),\n",
    "        name='bilstm'\n",
    "    )(combined)\n",
    "\n",
    "    output = layers.Dense(\n",
    "        num_diacritics,\n",
    "        activation='softmax',\n",
    "        name='diacritic_output'\n",
    "    )(lstm_out)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[char_input, word_input, position_input],\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "    der_metric = DERMetric(pad_id=0, space_id=pad_id)\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[der_metric]\n",
    "    )\n",
    "\n",
    "    print(\"\\nModel Architecture:\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "04088865",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(MODEL_PATH, custom_objects={\"DERMetric\": DERMetric})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1331ba8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m519s\u001b[0m 3s/step\n",
      "(5735, 999)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict({'char_input': x_test_char, 'word_input': x_test_word, 'position_input': x_test_position})\n",
    "\n",
    "y_pred_classes = np.argmax(y_pred, axis=-1)\n",
    "print(y_pred_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d91ebfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions after filtering padding and spaces: 237240\n",
      "Gold labels count: 182\n"
     ]
    }
   ],
   "source": [
    "id_to_diacritic = {v: k for k, v in diacritics_to_id.items()}\n",
    "id_to_char = {v: k for k, v in char_to_id.items()}\n",
    "\n",
    "space_id = char_to_id.get(' ', None)\n",
    "\n",
    "all_predictions = []\n",
    "for sample_idx in range(len(x_test_char)):\n",
    "    sample_chars = x_test_char[sample_idx]\n",
    "    sample_preds = y_pred_classes[sample_idx]\n",
    "    \n",
    "    non_padded_mask = (sample_chars != 0) & (sample_chars != char_to_id.get('PAD', 0)) & (sample_chars != char_to_id.get('UNK'))\n",
    "    \n",
    "    if space_id is not None:\n",
    "        non_space_mask = sample_chars != space_id\n",
    "        valid_mask = non_padded_mask & non_space_mask\n",
    "    else:\n",
    "        valid_mask = non_padded_mask\n",
    "    \n",
    "    valid_preds = sample_preds[valid_mask]\n",
    "    all_predictions.extend(valid_preds.tolist())\n",
    "\n",
    "y_pred_classes_filtered = np.array(all_predictions)\n",
    "df = pd.DataFrame({\n",
    "    \"ID\": np.arange(len(y_pred_classes_filtered)),\n",
    "    \"label\": y_pred_classes_filtered\n",
    "})\n",
    "\n",
    "# save to CSV\n",
    "df.to_csv(\"predictions.csv\", index=False)\n",
    "\n",
    "df_test = pd.read_csv(\"test_no_diacritics.csv\")\n",
    "mask = df_test[\"case_ending\"].astype(str).str.upper() == \"TRUE\"\n",
    "df_test_filtered = df_test[mask]\n",
    "\n",
    "df_pred_filtered = df.loc[df_test_filtered.index]\n",
    "df_pred_filtered.to_csv(\"predictions_case_ending.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Total predictions after filtering padding and spaces: {len(y_pred_classes_filtered)}\")\n",
    "print(f\"Gold labels count: {len(gold)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e56d69aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Mismatch in lengths!\n",
      "Predictions: 237240\n",
      "Gold labels: 182\n",
      "Difference: 237058\n",
      "\n",
      "First 10 predictions: [ 4 14 14  6  0  0 14  4 14  6]\n",
      "First 10 gold labels: [ 0  6  0  4  6  0  4 14  4  4]\n"
     ]
    }
   ],
   "source": [
    "if len(y_pred_classes_filtered) == len(gold):\n",
    "    accuracy = (gold['label'] == y_pred_classes_filtered).sum() / len(gold) * 100\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Test Set Evaluation\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total characters (excluding spaces): {len(gold)}\")\n",
    "    print(f\"Correct predictions: {(gold['label'] == y_pred_classes_filtered).sum()}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"DER (Diacritization Error Rate): {100 - accuracy:.2f}%\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(f\"ERROR: Mismatch in lengths!\")\n",
    "    print(f\"Predictions: {len(y_pred_classes_filtered)}\")\n",
    "    print(f\"Gold labels: {len(gold)}\")\n",
    "    print(f\"Difference: {abs(len(y_pred_classes_filtered) - len(gold))}\")\n",
    "    \n",
    "    print(\"\\nFirst 10 predictions:\", y_pred_classes_filtered[:10])\n",
    "    print(\"First 10 gold labels:\", gold['label'].values[:10])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545dc25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_diacritized_text(char_sequences, pred_sequences, char_to_id, id_to_diacritic, original_texts):\n",
    "    \"\"\"\n",
    "    Reconstruct the full diacritized text from character sequences and predictions.\n",
    "    \n",
    "    Args:\n",
    "        char_sequences: Array of character IDs\n",
    "        pred_sequences: Array of predicted diacritic IDs\n",
    "        char_to_id: Dictionary mapping characters to IDs\n",
    "        id_to_diacritic: Dictionary mapping IDs to diacritics\n",
    "        original_texts: List of original text segments\n",
    "    \n",
    "    Returns:\n",
    "        List of diacritized text segments\n",
    "    \"\"\"\n",
    "    id_to_char = {v: k for k, v in char_to_id.items()}\n",
    "    pad_id = char_to_id.get('<PAD>', 0)\n",
    "    \n",
    "    diacritized_texts = []\n",
    "    \n",
    "    for sample_idx, original_text in enumerate(original_texts):\n",
    "        sample_chars = char_sequences[sample_idx]\n",
    "        sample_preds = pred_sequences[sample_idx]\n",
    "        \n",
    "        diacritized_text = \"\"\n",
    "        for char_id, diac_id in zip(sample_chars, sample_preds):\n",
    "            if char_id == pad_id or char_id == 0:\n",
    "                diacritized_text += ' '\n",
    "                continue\n",
    "            \n",
    "            char = id_to_char.get(char_id, '')\n",
    "            diacritic = id_to_diacritic.get(diac_id, '')\n",
    "            \n",
    "            diacritized_text += char + diacritic\n",
    "        \n",
    "        diacritized_texts.append(diacritized_text)\n",
    "    \n",
    "    return diacritized_texts\n",
    "\n",
    "print(\"Reconstructing diacritized text...\")\n",
    "diacritized_results = reconstruct_diacritized_text(\n",
    "    x_test_char, \n",
    "    y_pred_classes, \n",
    "    char_to_id, \n",
    "    id_to_diacritic,\n",
    "    test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f96a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"diacritized_output.txt\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for text in diacritized_results:\n",
    "        f.write(text + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IP2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
